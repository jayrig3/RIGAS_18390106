[
    {
        "id": 1,
        "title": "Main Page",
        "content": "January 19\n The roadside hawk (Rupornis magnirostris) is a relatively small bird of prey in the family Accipitridae. It is found from Mexico through Central America and in most of South America east of the Andes. With the possible exception of dense rainforests, the roadside hawk is well adapted to most ecosystems in its range. It is also an urban bird, and is possibly the most common species of hawk seen in various cities throughout its range. This roadside hawk of the subspecies R. m. griseocauda was photographed feeding on a speckled racer in Orange Walk District, Belize.\n Photograph credit: Charles J. Sharp Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:\n This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.\n",
        "url": "https://en.wikipedia.org/wiki/Main_Page"
    },
    {
        "id": 2,
        "title": "Machine learning",
        "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.[2]\n ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[12]\n By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17]\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[19]\n Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20]\n As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]: 488 \n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]: 25 \n Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]\n There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[25][26][27]\n An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[28]\n According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[29] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[30]\n In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[31]\n Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[32]\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[34]\n Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[35] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[36] He also suggested the term data science as a placeholder to call the overall field.[36]\n Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[37]\n Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[38] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[39]\n Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[40] Statistical physics is thus finding applications in the area of medical diagnostics.[41]\n A core objective of a learner is to generalize from its experience.[5][42] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\n For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[43]\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n \n Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n Although each algorithm has advantages and limitations, no single algorithm works for all problems.[44][45][46]\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[47] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[48] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[18]\n Types of supervised-learning algorithms include active learning, classification and regression.[49] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. [50]\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[7] and density estimation.[51]\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[52][53]\n Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\n Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[57]\n Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[58] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[60]\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[62] and various forms of clustering.[63][64][65]\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[67]\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69]\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71]\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72]\n Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML).\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[76]\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n⇒\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[79]\n Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[84] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85]\n Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86]\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[87] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[88]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[89] which are inherently multi-dimensional.\n A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\n Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[91][92] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[93]\n The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[4][9] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[94]\n There are many applications for machine learning, including:\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[97] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[98] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[99] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[100] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[101] In 2019 Springer Nature published the first research book created using machine learning.[102] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[103] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[104] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[105][106][107] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[108]\n Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[109]\n Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[110][111][112] Other applications have been focusing on pre evacuation decisions in building fires.[113][114]\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[115][116][117] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[118]\n The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[119] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[119]\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[120] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[121][122] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[123]\n Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[124]\n Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[125] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[126] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[127]\n Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[128] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[129][130]\n Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[131] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[132]\n Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[133][134][135]\n Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[136]\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[137]\n The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[138] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[138]\n Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[139]\n Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[140] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[139] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[141][142] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[143]\n While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[144] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[145] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[145]\n Language models learned from data have been shown to contain human-like biases.[146][147] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[148][149] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[150]\n In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants.\"[143] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas.[151] Similar issues with recognizing non-white people have been found in many other systems.[152]\n Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[153] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[154]\n There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[155]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[156] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[157] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[158][159]\n Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures.[160]\n A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[161][162]\n Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[163][164][165] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[166][167] approximate computing,[168] and model optimization.[169][170] Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing.\n Software suites containing a variety of machine learning algorithms include the following:\n",
        "url": "https://en.wikipedia.org/wiki/Machine_learning"
    },
    {
        "id": 3,
        "title": "Machine Learning (journal)",
        "content": "Machine Learning  is a peer-reviewed scientific journal, published since 1986.\n In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1]\n Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.[2]\n \n This article about a computer science journal is a stub. You can help Wikipedia by expanding it. See tips for writing articles about academic journals. Further suggestions might be found on the article's talk page.",
        "url": "https://en.wikipedia.org/wiki/Machine_Learning_(journal)"
    },
    {
        "id": 4,
        "title": "Statistical learning in language acquisition",
        "content": "Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\n The earliest evidence for these statistical learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable \"pseudowords\" that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing \"pseudowords\" as opposed to \"nonwords\" from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words. [citation needed]\n Since the initial discovery of the role of statistical learning in lexical acquisition, the same mechanism has been proposed for elements of phonological acquisition, and syntactical acquisition, as well as in non-linguistic domains. Further research has also indicated that statistical learning is likely a domain-general and even species-general learning mechanism, occurring for visual as well as auditory information, and in both primates and non-primates.\n The role of statistical learning in language acquisition has been particularly well documented in the area of lexical acquisition.[1] One important contribution to infants' understanding of segmenting words from a continuous stream of speech is their ability to recognize statistical regularities of the speech heard in their environments.[1] Although many factors play an important role, this specific mechanism is powerful and can operate over a short time scale.[1]\n It is a well-established finding that, unlike written language, spoken language does not have any clear boundaries between words; spoken language is a continuous stream of sound rather than individual words with silences between them.[2] This lack of segmentation between linguistic units presents a problem for young children learning language, who must be able to pick out individual units from the continuous speech streams that they hear.[3] One proposed method of how children are able to solve this problem is that they are attentive to the statistical regularities of the world around them.[2][3] For example, in the phrase \"pretty baby\", children are more likely to hear the sounds pre and ty heard together during the entirety of the lexical input around them than they are to hear the sounds ty and ba together.[3] In an artificial grammar learning study with adult participants, Saffran, Newport, and Aslin found that participants were able to locate word boundaries based only on transitional probabilities, suggesting that adults are capable of using statistical regularities in a language-learning task.[4] This is a robust finding that has been widely replicated.[1]\n To determine if young children have these same abilities Saffran Aslin and Newport exposed 8-month-old infants to an artificial grammar.[3] The grammar was composed of four words, each composed of three nonsense syllables. During the experiment, infants heard a continuous speech stream of these words. The speech was presented in a monotone with no cues (such as pauses, intonation, etc.) to word boundaries other than the statistical probabilities. Within a word, the transitional probability of two syllable pairs was 1.0: in the word bidaku, for example, the probability of hearing the syllable da immediately after the syllable bi was 100%. Between words, however, the transitional probability of hearing a syllable pair was much lower: After any given word (e.g., bidaku) was presented, one of three words could follow (in this case, padoti, golabu, or tupiro), so the likelihood of hearing any given syllable after ku was only 33%.\n To determine if infants were picking up on the statistical information, each infant was presented with multiple presentations of either a word from the artificial grammar or a nonword made up of the same syllables but presented in a random order. Infants who were presented with nonwords during the test phase listened significantly longer to these words than infants who were presented with words from the artificial grammar, showing a novelty preference for these new nonwords. However, the implementation of the test could also be due to infants learning serial-order information and not to actually learning transitional probabilities between words. That is, at test, infants heard strings such as dapiku and tilado that were never presented during learning; they could simply have learned that the syllable ku never followed the syllable pi.[3]\n To look more closely at this issue, Saffran Aslin and Newport conducted another study in which infants underwent the same training with the artificial grammar but then were presented with either words or part-words rather than words or nonwords.[3] The part-words were syllable sequences composed of the last syllable from one word and the first two syllables from another (such as kupado). Because the part-words had been heard during the time when children were listening to the artificial grammar, preferential listening to these part-words would indicate that children were learning not only serial-order information, but also the statistical likelihood of hearing particular syllable sequences. Again, infants showed greater listening times to the novel (part-) words, indicating that 8-month-old infants were able to extract these statistical regularities from a continuous speech stream.\n This result has been the impetus for much more research on the role of statistical learning in lexical acquisition and other areas (see[1]). In a follow-up to the original report,[3] Aslin, Saffran, and Newport found that even when words and part words occurred equally often in the speech stream, but with different transitional probabilities between syllables of words and part words, infants were still able to detect the statistical regularities and still preferred to listen to the novel part-words over the familiarized words.[5] This finding provides stronger evidence that infants are able to pick up transitional probabilities from the speech they hear, rather than just being aware of frequencies of individual syllable sequences.[1]\n Another follow-up study examined the extent to which the statistical information learned during this type of artificial grammar learning feeds into knowledge that infants may already have about their native language.[6] Infants preferred to listen to words over part-words, whereas there was no significant difference in the nonsense frame condition. This finding suggests that even pre-linguistic infants are able to integrate the statistical cues they learn in a laboratory into their previously acquired knowledge of a language.[1][6] In other words, once infants have acquired some linguistic knowledge, they incorporate newly acquired information into that previously acquired learning.\n A related finding indicates that slightly older infants can acquire both lexical and grammatical regularities from a single set of input,[7] suggesting that they are able to use outputs of one type of statistical learning (cues that lead to the discovery of word boundaries) as input to a second type (cues that lead to the discovery of syntactical regularities.[1][7] At test, 12-month-olds preferred to listen to sentences that had the same grammatical structure as the artificial language they had been tested on rather than sentences that had a different (ungrammatical) structure. Because learning grammatical regularities requires infants to be able to determine boundaries between individual words, this indicates that infants who are still quite young are able to acquire multiple levels of language knowledge (both lexical and syntactical) simultaneously, indicating that statistical learning is a powerful mechanism at play in language learning.[1][7]\n Despite the large role that statistical learning appears to play in lexical acquisition, it is likely not the only mechanism by which infants learn to segment words. Statistical learning studies are generally conducted with artificial grammars that have no cues to word boundary information other than transitional probabilities between words. Real speech, though, has many different types of cues to word boundaries, including prosodic and phonotactic information.[8]\n Together, the findings from these studies of statistical learning in language acquisition indicate that statistical properties of the language are a strong cue in helping infants learn their first language.[1]\n There is much evidence that statistical learning is an important component of both discovering which phonemes are important for a given language and which contrasts within phonemes are important.[9][10][11] Having this knowledge is important for aspects of both speech perception and speech production.\n Since the discovery of infants' statistical learning abilities in word learning, the same general mechanism has also been studied in other facets of language learning. For example, it is well-established that infants can discriminate between phonemes of many different languages but eventually become unable to discriminate between phonemes that do not appear in their native language;[12] however, it was not clear how this decrease in discriminatory ability came about. Maye et al. suggested that the mechanism responsible might be a statistical learning mechanism in which infants track the distributional regularities of the sounds in their native language.[12] To test this idea, Maye et al. exposed 6- and 8-month-old infants to a continuum of speech sounds that varied on the degree to which they were voiced. The distribution that the infants heard was either bimodal, with sounds from both ends of the voicing continuum heard most often, or unimodal, with sounds from the middle of the distribution heard most often. The results indicated that infants from both age groups were sensitive to the distribution of phonemes. At test, infants heard either non-alternating (repeated exemplars of tokens 3 or 6 from an 8-token continuum) or alternating (exemplars of tokens 1 and 8) exposures to specific phonemes on the continuum. Infants exposed to the bimodal distribution listened longer to the alternating trials than the non-alternating trials while there was no difference in listening times for infants exposed to the unimodal distribution. This finding indicates that infants exposed the bimodal distribution were better able to discriminate sounds from the two ends of the distribution than were infants in the unimodal condition, regardless of age. This type of statistical learning differs from that used in lexical acquisition, as it requires infants to track frequencies rather than transitional probabilities, and has been named \"distributional learning\".[10]\n Distributional learning has also been found to help infants contrast two phonemes that they initially have difficulty in discriminating between. Maye, Weiss, and Aslin found that infants who were exposed to a bimodal distribution of a non-native contrast that was initially difficult to discriminate were better able to discriminate the contrast than infants exposed to a unimodal distribution of the same contrast.[13] Maye et al. also found that infants were able to abstract features of a contrast (i.e., voicing onset time) and generalize that feature to the same type of contrast at a different place of articulation, a finding that has not been found in adults.\n In a review of the role of distributional learning on phonological acquisition, Werker et al. note that distributional learning cannot be the only mechanism by which phonetic categories are acquired.[10] However, it does seem clear that this type of statistical learning mechanism can play a role in this skill, although research is ongoing.[10]\n A related finding regarding statistical cues to phonological acquisition is a phenomenon known as the perceptual magnet effect.[14][15][16] In this effect, a prototypical phoneme of a person's native language acts as a \"magnet\" for similar phonemes, which are perceived as belonging to the same category as the prototypical phoneme. In the original test of this effect, adult participants were asked to indicate if a given exemplar of a particular phoneme differed from a referent phoneme.[14] If the referent phoneme is a non-prototypical phoneme for that language, both adults and 6-month-old infants show less generalization to other sounds than they do for prototypical phonemes, even if the subjective distance between the sounds is the same.[14][16] That is, adults and infants are both more likely to notice that a particular phoneme differs from the referent phoneme if that referent phoneme is a non-prototypical exemplar than if it is a prototypical exemplar. The prototypes themselves are apparently discovered through a distributional learning process, in which infants are sensitive to the frequencies with which certain sounds occur and treat those that occur most often as the prototypical phonemes of their language.[11]\n A statistical learning device has also been proposed as a component of syntactical acquisition for young children.[1][9][17] Early evidence for this mechanism came largely from studies of computer modeling or analyses of natural language corpora.[18][19] These early studies focused largely on distributional information specifically rather than statistical learning mechanisms generally. Specifically, in these early papers it was proposed that children created templates of possible sentence structures involving unnamed categories of word types (i.e., nouns or verbs, although children would not put these labels on their categories). Children were thought to learn which words belonged to the same categories by tracking the similar contexts in which words of the same category appeared.\n Later studies expanded these results by looking at the actual behavior of children or adults who had been exposed to artificial grammars.[9] These later studies also considered the role of statistical learning more broadly than the earlier studies, placing their results in the context of the statistical learning mechanisms thought to be involved with other aspects of language learning, such as lexical acquisition.\n Evidence from a series of four experiments conducted by Gomez and Gerken suggests that children are able to generalize grammatical structures with less than two minutes of exposure to an artificial grammar.[9][20] In the first experiment, 11-12 month-old infants were trained on an artificial grammar composed of nonsense words with a set grammatical structure. At test, infants heard both novel grammatical and ungrammatical sentences. Infants oriented longer toward the grammatical sentences, in line with previous research that suggests that infants generally orient for a longer amount of time to natural instances of language rather than altered instances of language e.g.,.[21] (This familiarity preference differs from the novelty preference generally found in word-learning studies, due to the differences between lexical acquisition and syntactical acquisition.) This finding indicates that young children are sensitive to the grammatical structure of language even after minimal exposure. Gomez and Gerken also found that this sensitivity is evident when ungrammatical transitions are located in the middle of the sentence (unlike in the first experiment, in which all the errors occurred at the beginning and end of the sentences), that the results could not be due to an innate preference for the grammatical sentences caused by something other than grammar, and that children are able to generalize the grammatical rules to new vocabulary.\n Together these studies suggest that infants are able to extract a substantial amount of syntactic knowledge even from limited exposure to a language.[9][20] Children apparently detected grammatical anomalies whether the grammatical violation in the test sentences occurred at the end or in the middle of the sentence. Additionally, even when the individual words of the grammar were changed, infants were still able to discriminate between grammatical and ungrammatical strings during the test phase. This generalization indicates that infants were not learning vocabulary-specific grammatical structures, but abstracting the general rules of that grammar and applying those rules to novel vocabulary. Furthermore, in all four experiments, the test of grammatical structures occurred five minutes after the initial exposure to the artificial grammar had ended, suggesting that the infants were able to maintain the grammatical abstractions they had learned even after a short delay.\n In a similar study, Saffran found that adults and older children (first- and second grade children) were also sensitive to syntactical information after exposure to an artificial language which had no cues to phrase structure other than the statistical regularities that were present.[22] Both adults and children were able to pick out sentences that were ungrammatical at a rate greater than chance, even under an \"incidental\" exposure condition in which participants' primary goal was to complete a different task while hearing the language.\n Although the number of studies dealing with statistical learning of syntactical information is limited, the available evidence does indicate that the statistical learning mechanisms are likely a contributing factor to children's ability to learn their language.[9][17]\n Much of the early work using statistical learning paradigms focused on the ability for children or adults to learn a single language,[1] consistent with the process of language acquisition for monolingual speakers or learners. However, it is estimated that approximately 60-75% of people in the world are bilingual.[23] More recently, researchers have begun looking at the role of statistical learning for those who speak more than one language. Although there are no reviews on this topic yet, Weiss, Gerfen, and Mitchel examined how hearing input from multiple artificial languages simultaneously can affect the ability to learn either or both languages.[24] Over four experiments, Weiss et al. found that, after exposure to two artificial languages, adult learners are capable of determining word boundaries in both languages when each language is spoken by a different speaker. However, when the two languages were spoken by the same speaker, participants were able learn both languages only when they were \"congruent\"—when the word boundaries of one language matched the word boundaries of the other. When the languages were incongruent—a syllable that appeared in the middle of a word in one language appeared at the end of the word in the other language—and spoken by a single speaker, participants were able to learn, at best, one of the two languages. A final experiment showed that the inability to learn incongruent languages spoken in the same voice was not due to syllable overlap between the languages but due to differing word boundaries.\n Similar work replicates the finding that learners are able to learn two sets of statistical representations when an additional cue is present (two different male voices in this case).[25] In their paradigm, the two languages were presented consecutively, rather than interleaved as in Weiss et al.'s paradigm,[24] and participants did learn the first artificial language to which they had been exposed better than the second, although participants' performance was above chance for both languages.\n While statistical learning improves and strengthens multilingualism, it appears that the inverse is not true. In a study by Yim and Rudoy[26] it was found that both monolingual and bilingual children perform statistical learning tasks equally well.\n Antovich and Graf Estes[27] found that 14-month-old bilingual children are better than monolinguals at segmenting two different artificial languages using transitional probability cues. They suggest that a bilingual environment in early childhood trains children to rely on statistical regularities to segment the speech flow and access two lexical systems.\n A statistical learning mechanism has also been proposed for learning the meaning of words. Specifically, Yu and Smith conducted a pair of studies in which adults were exposed to pictures of objects and heard nonsense words.[28] Each nonsense word was paired with a particular object. There were 18 total word-referent pairs, and each participant was presented with either 2, 3, or 4 objects at a time, depending on the condition, and heard the nonsense word associated with one of those objects. Each word-referent pair was presented 6 times over the course of the training trials; after the completion of the training trials, participants completed a forced-alternative test in which they were asked to choose the correct referent that matched a nonsense word they were given. Participants were able to choose the correct item more often than would happen by chance, indicating, according to the authors, that they were using statistical learning mechanisms to track co-occurrence probabilities across training trials.\n An alternative hypothesis is that learners in this type of task may be using a \"propose-but-verify\" mechanism rather than a statistical learning mechanism.[29][30] Medina et al. and Trueswell et al. argue that, because Yu and Smith only tracked knowledge at the end of the training, rather than tracking knowledge on a trial-by-trial basis, it is impossible to know if participants were truly updating statistical probabilities of co-occurrence (and therefore maintaining multiple hypotheses simultaneously), or if, instead, they were forming a single hypothesis and checking it on the next trial.[28][29][30] For example, if a participant is presented with a picture of a dog and a picture of a shoe, and hears the nonsense word vash she might hypothesize that vash refers to the dog. On a future trial, she may see a picture of a shoe and a picture of a door and again hear the word vash. If statistical learning is the mechanism by which word-referent mappings are learned, then the participant would be more likely to select the picture of the shoe than the door, as shoe would have appeared in conjunction with the word vash 100% of the time. However, if participants are simply forming a single hypothesis, they may fail to remember the context of the previous presentation of vash (especially if, as in the experimental conditions, there are multiple trials with other words in between the two presentations of vash) and therefore be at chance in this second trial. According to this proposed mechanism of word learning, if the participant had correctly guessed that vash referred to the shoe in the first trial, her hypothesis would be confirmed in the subsequent trial.\n To distinguish between these two possibilities, Trueswell et al. conducted a series of experiments similar to those conducted by Yu and Smith except that participants were asked to indicate their choice of the word-referent mapping on each trial, and only a single object name was presented on each trial (with varying numbers of objects).[28][30] Participants would therefore have been at chance when they are forced to make a choice in their first trial. The results from the subsequent trials indicate that participants were not using a statistical learning mechanism in these experiments, but instead were using a propose-and-verify mechanism, holding only one potential hypothesis in mind at a time. Specifically, if participants had chosen an incorrect word-referent mapping in an initial presentation of a nonsense word (from a display of five possible choices), their likelihood of choosing the correct word-referent mapping in the next trial of that word was still at chance, or 20%. If, though, the participant had chosen the correct word-referent mapping on an initial presentation of a nonsense word, the likelihood of choosing the correct word-referent mapping on the subsequent presentation of that word was approximately 50%. These results were also replicated in a condition where participants were choosing between only two alternatives. These results suggest that participants did not remember the surrounding context of individual presentations and were therefore not using statistical cues to determine the word-referent mappings. Instead, participants make a hypothesis regarding a word-referent mapping and, on the next presentation of that word, either confirm or reject the hypothesis accordingly.\n Overall, these results, along with similar results from Medina et al., indicate that word meanings may not be learned through a statistical learning mechanism in these experiments, which ask participants to hypothesize a mapping even on the first occurrence (i.e., not cross-situationally).[29] However, when the propose-but-verify mechanism has been compared to a statistical learning mechanism, the former was unable to reproduce individual learning trajectories nor fit as well as the latter.[31]\n Additionally, statistical learning by itself cannot account even for those aspects of language acquisition for which it has been shown to play a large role. For example, Kuhl, Tsao, and Liu found that young English-learning infants who spent time in a laboratory session with a native Mandarin speaker were able to distinguish between phonemes that occur in Mandarin but not in English, unlike infants who were in a control condition.[32] Infants in this control condition came to the lab as often as infants in the experimental condition, but were exposed only to English; when tested at a later date, they were unable to distinguish the Mandarin phonemes. In a second experiment, the authors presented infants with audio or audiovisual recordings of Mandarin speakers and tested the infants' ability to distinguish between the Mandarin phonemes. In this condition, infants failed to distinguish the foreign language phonemes. This finding indicates that social interaction is a necessary component of language learning and that, even if infants are presented with the raw data of hearing a language, they are unable to take advantage of the statistical cues present in that data if they are not also experiencing the social interaction.[11]\n Although the phenomenon of statistical learning was first discovered in the context of language acquisition and there is much evidence of its role in that purpose, work since the original discovery has suggested that statistical learning may be a domain general skill and is likely not unique to humans.[3][33] For example, Saffran, Johnson, Aslin, and Newport found that both adults and infants were able to learn statistical probabilities of \"words\" created by playing different musical tones (i.e., participants heard the musical notes D, E, and F presented together during training and were able to recognize those notes as a unit at test as compared to three notes that had not been presented together).[34] In non-auditory domains, there is evidence that humans are able to learn statistical visual information whether that information is presented across space, e.g.,[35] or time, e.g.,.[36] Evidence of statistical learning has also been found in other primates, e.g.,[37] and some limited statistical learning abilities have been found even in non-primates like rats.[38] Together these findings suggest that statistical learning may be a generalized learning mechanism that happens to be utilized in language acquisition, rather than a mechanism that is unique to the human infant's ability to learn his or her language(s).\n Further evidence for domain general statistical learning was suggested in a study run through the University of Cornell Department of Psychology concerning visual statistical learning in infancy. Researchers in this study questioned whether domain generality of statistical learning in infancy would be seen using visual information. After first viewing images in statistically predictable patterns, infants were then exposed to the same familiar patterns in addition to novel sequences of the same identical stimulus components. Interest in the visuals was measured by the amount of time the child looked at the stimuli in which the researchers named \"looking time\". All ages of infant participants showed more interest in the novel sequence relative to the familiar sequence. In demonstrating a preference for the novel sequences (which violated the transitional probability that defined the grouping of the original stimuli) the results of the study support the likelihood of domain general statistical learning in infancy.[39]\n",
        "url": "https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition"
    },
    {
        "id": 5,
        "title": "Data mining",
        "content": "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\n The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.\n The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.\n The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[8]\n The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[9][10] Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).\n The term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego–based company, to pitch their Database Mining Workstation;[11] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in the AI and machine learning communities. However, the term data mining became more popular in the business and press communities.[12] Currently, the terms data mining and knowledge discovery are used interchangeably.\n The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s).[13] The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[14] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.\n The knowledge discovery in databases (KDD) process is commonly defined with the stages:\n It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:\n or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.\n Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[15][16][17][18]\n The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]\n Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\n Data mining involves six common classes of tasks:[5]\n Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.[21]\n The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.\n If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\n The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[25]\n Computer science conferences on data mining include:\n Data mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases.\n There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\n For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]\n Data mining is used wherever there is digital data available. Notable examples of data mining can be found throughout business, medicine, science, finance, construction, and surveillance.\n While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise).[27]\n The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]\n Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32]\n It is recommended[according to whom?] to be aware of the following before data are collected:[31]\n Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even \"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[33]\n The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\nemotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\nprescription information to data mining companies who in turn provided the data\nto pharmaceutical companies.[34]\n Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[35]\n In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[36]\n In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\"[37] This underscores the necessity for data anonymity in data aggregation and mining practices.\n U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\n Under European copyright database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist, so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[38] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020.[39]\n The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]\n US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.[42]\n The following applications are available under free/open-source licenses. Public access to application source code is also available.\n The following applications are available under proprietary licenses.\n For more information about extracting information out of data (as opposed to analyzing data), see:\n",
        "url": "https://en.wikipedia.org/wiki/Data_mining"
    },
    {
        "id": 6,
        "title": "Supervised learning",
        "content": "In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values.[1] An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error.\n To solve a given problem of supervised learning, the following steps must be performed:\n A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\n There are four major issues to consider in supervised learning:\n A first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. A learning algorithm has high variance for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\n The second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance.\n A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\n A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\n In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]\n Other factors to consider when choosing and applying a learning algorithm include the following:\n When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\n The most widely used learning algorithms are: \n Given a set of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n training examples of the form \n\n\n\n{\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n.\n.\n.\n,\n(\n\nx\n\nN\n\n\n,\n\n\ny\n\nN\n\n\n)\n}\n\n\n{\\displaystyle \\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}}\n\n such that \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n is the feature vector of the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th example and \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n is its label (i.e., class), a learning algorithm seeks a function \n\n\n\ng\n:\nX\n→\nY\n\n\n{\\displaystyle g:X\\to Y}\n\n, where \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the input space and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is the output space. The function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is an element of some space of possible functions \n\n\n\nG\n\n\n{\\displaystyle G}\n\n, usually called the hypothesis space. It is sometimes convenient to represent \n\n\n\ng\n\n\n{\\displaystyle g}\n\n using a scoring function \n\n\n\nf\n:\nX\n×\nY\n→\n\nR\n\n\n\n{\\displaystyle f:X\\times Y\\to \\mathbb {R} }\n\n such that \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as returning the \n\n\n\ny\n\n\n{\\displaystyle y}\n\n value that gives the highest score: \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n⁡\nmax\n\ny\n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;f(x,y)}\n\n. Let \n\n\n\nF\n\n\n{\\displaystyle F}\n\n denote the space of scoring functions.\n Although \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and \n\n\n\nF\n\n\n{\\displaystyle F}\n\n can be any space of functions, many learning algorithms are probabilistic models where \n\n\n\ng\n\n\n{\\displaystyle g}\n\n takes the form of a conditional probability model \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n⁡\nmax\n\ny\n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;P(y|x)}\n\n, or \n\n\n\nf\n\n\n{\\displaystyle f}\n\n takes the form of a joint probability model \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\n There are two basic approaches to choosing \n\n\n\nf\n\n\n{\\displaystyle f}\n\n or \n\n\n\ng\n\n\n{\\displaystyle g}\n\n: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\n In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n. In order to measure how well a function fits the training data, a loss function \n\n\n\nL\n:\nY\n×\nY\n→\n\n\nR\n\n\n≥\n0\n\n\n\n\n{\\displaystyle L:Y\\times Y\\to \\mathbb {R} ^{\\geq 0}}\n\n is defined. For training example \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n, the loss of predicting the value \n\n\n\n\n\n\ny\n^\n\n\n\n\n\n{\\displaystyle {\\hat {y}}}\n\n is \n\n\n\nL\n(\n\ny\n\ni\n\n\n,\n\n\n\ny\n^\n\n\n\n)\n\n\n{\\displaystyle L(y_{i},{\\hat {y}})}\n\n.\n The risk \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n of function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as the expected loss of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n. This can be estimated from the training data as\n In empirical risk minimization, the supervised learning algorithm seeks the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n When \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a conditional probability distribution \n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle P(y|x)}\n\n and the loss function is the negative log likelihood: \n\n\n\nL\n(\ny\n,\n\n\n\ny\n^\n\n\n\n)\n=\n−\nlog\n⁡\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle L(y,{\\hat {y}})=-\\log P(y|x)}\n\n, then empirical risk minimization is equivalent to maximum likelihood estimation.\n When \n\n\n\nG\n\n\n{\\displaystyle G}\n\n contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well (overfitting).\n Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\n A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a linear function of the form\n A popular regularization penalty is \n\n\n\n\n∑\n\nj\n\n\n\nβ\n\nj\n\n\n2\n\n\n\n\n{\\displaystyle \\sum _{j}\\beta _{j}^{2}}\n\n, which is the squared Euclidean norm of the weights, also known as the \n\n\n\n\nL\n\n2\n\n\n\n\n{\\displaystyle L_{2}}\n\n norm. Other norms include the \n\n\n\n\nL\n\n1\n\n\n\n\n{\\displaystyle L_{1}}\n\n norm, \n\n\n\n\n∑\n\nj\n\n\n\n|\n\n\nβ\n\nj\n\n\n\n|\n\n\n\n{\\displaystyle \\sum _{j}|\\beta _{j}|}\n\n, and the \n\n\n\n\nL\n\n0\n\n\n\n\n{\\displaystyle L_{0}}\n\n \"norm\", which is the number of non-zero \n\n\n\n\nβ\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\ns. The penalty will be denoted by \n\n\n\nC\n(\ng\n)\n\n\n{\\displaystyle C(g)}\n\n.\n The supervised learning optimization problem is to find the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes\n The parameter \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n controls the bias-variance tradeoff. When \n\n\n\nλ\n=\n0\n\n\n{\\displaystyle \\lambda =0}\n\n, this gives empirical risk minimization with low bias and high variance. When \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n is large, the learning algorithm will have high bias and low variance. The value of \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n can be chosen empirically via  cross-validation.\n The complexity penalty has a Bayesian interpretation as the negative log prior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n, \n\n\n\n−\nlog\n⁡\nP\n(\ng\n)\n\n\n{\\displaystyle -\\log P(g)}\n\n, in which case \n\n\n\nJ\n(\ng\n)\n\n\n{\\displaystyle J(g)}\n\n is the posterior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n The training methods described above are discriminative training methods, because they seek to find a function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that discriminates well between the different output values (see discriminative model). For the special case where \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n is a joint probability distribution and the loss function is the negative log likelihood \n\n\n\n−\n\n∑\n\ni\n\n\nlog\n⁡\nP\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n,\n\n\n{\\displaystyle -\\sum _{i}\\log P(x_{i},y_{i}),}\n\n a risk minimization algorithm is said to perform generative training, because \n\n\n\nf\n\n\n{\\displaystyle f}\n\n can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\n There are several ways in which the standard supervised learning problem can be generalized:\n",
        "url": "https://en.wikipedia.org/wiki/Supervised_learning"
    },
    {
        "id": 7,
        "title": "Unsupervised learning",
        "content": "Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data.[1] Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.[2]\n Conceptually, unsupervised learning divides into the aspects of data, training, algorithm, and downstream applications. Typically, the dataset is harvested cheaply \"in the wild\", such as massive text corpus obtained by web crawling, with only minor filtering (such as Common Crawl). This compares favorably to supervised learning, where the dataset (such as the ImageNet1000) is typically constructed manually, which is much more expensive.\n There were algorithms designed specifically for unsupervised learning, such as clustering algorithms like k-means, dimensionality reduction techniques like principal component analysis (PCA), Boltzmann machine learning, and autoencoders. After the rise of deep learning, most large-scale unsupervised learning have been done by training general-purpose neural network architectures by gradient descent, adapted to performing unsupervised learning by designing an appropriate training procedure.\n Sometimes a trained model can be used as-is, but more often they are modified for downstream applications. For example, the generative pretraining method trains a model to generate a textual dataset, before finetuning it for other applications, such as text classification.[3][4] As another example, autoencoders are trained to good features, which can then be used as a module for other models, such as in a latent diffusion model.\n Tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, ReLU, and adaptive learning rates.\n A typical generative task is as follows. At each step, a datapoint is sampled from the dataset, and part of the data is removed, and the model must infer the removed part. This is particularly clear for the denoising autoencoders and BERT.\n During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (i.e. correct its weights and biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.\n In contrast to supervised methods' dominant use of backpropagation, unsupervised learning also employs other methods  including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. See the table below for more details.\n An energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion \n\n\n\np\n∝\n\ne\n\n−\nE\n\n/\n\nk\nT\n\n\n\n\n{\\displaystyle p\\propto e^{-E/kT}}\n\n, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is \n\n\n\np\n=\n\ne\n\n−\nE\n\n\n\n/\n\nZ\n\n\n{\\displaystyle p=e^{-E}/Z}\n\n,[5] where \n\n\n\np\n\n\n{\\displaystyle p}\n\n and \n\n\n\nE\n\n\n{\\displaystyle E}\n\n vary over every possible activation pattern and \n\n\n\n\n\nZ\n=\n\n∑\n\n\n\nAll Patterns\n\n\n\n\n\ne\n\n−\nE\n(\n\npattern\n\n)\n\n\n\n\n\n\n{\\displaystyle \\textstyle {Z=\\sum _{\\scriptscriptstyle {\\text{All Patterns}}}e^{-E({\\text{pattern}})}}}\n\n. To be more precise, \n\n\n\np\n(\na\n)\n=\n\ne\n\n−\nE\n(\na\n)\n\n\n\n/\n\nZ\n\n\n{\\displaystyle p(a)=e^{-E(a)}/Z}\n\n, where \n\n\n\na\n\n\n{\\displaystyle a}\n\n is an activation pattern of all neurons (visible and hidden). Hence, some early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls \n\n\n\n−\nE\n\n\n\n{\\displaystyle -E\\,}\n\n the Harmony. A network seeks low energy which is high Harmony.\n This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks.  Circles are neurons and edges between them are connection weights.  As network design changes, features are added on to enable new capabilities or removed to make learning faster.  For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).\n Of the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.\n Here, we highlight some characteristics of select networks.  The details of each are given in the comparison table below. \n The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together.[8] In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons.[9] A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.[10]\n Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.[11] Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n A central application of unsupervised learning is in the field of density estimation in statistics,[12] though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n In particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[15]\n The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\n",
        "url": "https://en.wikipedia.org/wiki/Unsupervised_learning"
    },
    {
        "id": 8,
        "title": "Weak supervision",
        "content": "Weak supervision (also known as semi-supervised learning) is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. \n The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.\n More formally, semi-supervised learning assumes a set of \n\n\n\nl\n\n\n{\\displaystyle l}\n\n independently identically distributed examples \n\n\n\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nl\n\n\n∈\nX\n\n\n{\\displaystyle x_{1},\\dots ,x_{l}\\in X}\n\n with corresponding labels \n\n\n\n\ny\n\n1\n\n\n,\n…\n,\n\ny\n\nl\n\n\n∈\nY\n\n\n{\\displaystyle y_{1},\\dots ,y_{l}\\in Y}\n\n and \n\n\n\nu\n\n\n{\\displaystyle u}\n\n unlabeled examples \n\n\n\n\nx\n\nl\n+\n1\n\n\n,\n…\n,\n\nx\n\nl\n+\nu\n\n\n∈\nX\n\n\n{\\displaystyle x_{l+1},\\dots ,x_{l+u}\\in X}\n\n are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.\n Semi-supervised learning may refer to either transductive learning or inductive learning.[1] The goal of transductive learning is to infer the correct labels for the given unlabeled data \n\n\n\n\nx\n\nl\n+\n1\n\n\n,\n…\n,\n\nx\n\nl\n+\nu\n\n\n\n\n{\\displaystyle x_{l+1},\\dots ,x_{l+u}}\n\n only. The goal of inductive learning is to infer the correct mapping from \n\n\n\nX\n\n\n{\\displaystyle X}\n\n to \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n.\n It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.\n In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:[2]\n Points that are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.[3]\n The data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.\n The data lie approximately on a manifold of much lower dimension than the input space. In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.\n The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,[4] and images of various facial expressions are controlled by a few muscles. In these cases, it is better to consider distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images, respectively.\n The heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning,[2] with examples of applications starting in the 1960s.[5]\n The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s.[6] Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.[7]\n Generative approaches to statistical learning first seek to estimate \n\n\n\np\n(\nx\n\n|\n\ny\n)\n\n\n{\\displaystyle p(x|y)}\n\n,[disputed – discuss] the distribution of data points belonging to each class. The probability \n\n\n\np\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle p(y|x)}\n\n that a given point \n\n\n\nx\n\n\n{\\displaystyle x}\n\n has label \n\n\n\ny\n\n\n{\\displaystyle y}\n\n is then proportional to \n\n\n\np\n(\nx\n\n|\n\ny\n)\np\n(\ny\n)\n\n\n{\\displaystyle p(x|y)p(y)}\n\n by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about \n\n\n\np\n(\nx\n)\n\n\n{\\displaystyle p(x)}\n\n) or as an extension of unsupervised learning (clustering plus some labels).\n Generative models assume that the distributions take some particular form \n\n\n\np\n(\nx\n\n|\n\ny\n,\nθ\n)\n\n\n{\\displaystyle p(x|y,\\theta )}\n\n parameterized by the vector \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.[8] \nHowever, if the assumptions are correct, then the unlabeled data necessarily improves performance.[7]\n The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.\n The parameterized joint distribution can be written as \n\n\n\np\n(\nx\n,\ny\n\n|\n\nθ\n)\n=\np\n(\ny\n\n|\n\nθ\n)\np\n(\nx\n\n|\n\ny\n,\nθ\n)\n\n\n{\\displaystyle p(x,y|\\theta )=p(y|\\theta )p(x|y,\\theta )}\n\n by using the chain rule. Each parameter vector \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n is associated with a decision function \n\n\n\n\nf\n\nθ\n\n\n(\nx\n)\n=\n\n\nargmax\ny\n\n\n \np\n(\ny\n\n|\n\nx\n,\nθ\n)\n\n\n{\\displaystyle f_{\\theta }(x)={\\underset {y}{\\operatorname {argmax} }}\\ p(y|x,\\theta )}\n\n. \nThe parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n:\n Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss \n\n\n\n(\n1\n−\ny\nf\n(\nx\n)\n\n)\n\n+\n\n\n\n\n{\\displaystyle (1-yf(x))_{+}}\n\n for labeled data, a loss function \n\n\n\n(\n1\n−\n\n|\n\nf\n(\nx\n)\n\n|\n\n\n)\n\n+\n\n\n\n\n{\\displaystyle (1-|f(x)|)_{+}}\n\n is introduced over the unlabeled data by letting \n\n\n\ny\n=\nsign\n⁡\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle y=\\operatorname {sign} {f(x)}}\n\n. TSVM then selects \n\n\n\n\nf\n\n∗\n\n\n(\nx\n)\n=\n\nh\n\n∗\n\n\n(\nx\n)\n+\nb\n\n\n{\\displaystyle f^{*}(x)=h^{*}(x)+b}\n\n from a reproducing kernel Hilbert space \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n by minimizing the regularized empirical risk:\n An exact solution is intractable due to the non-convex term \n\n\n\n(\n1\n−\n\n|\n\nf\n(\nx\n)\n\n|\n\n\n)\n\n+\n\n\n\n\n{\\displaystyle (1-|f(x)|)_{+}}\n\n, so research focuses on useful approximations.[9]\n Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).\n Laplacian regularization has been historically approached through graph-Laplacian.\nGraph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its \n\n\n\nk\n\n\n{\\displaystyle k}\n\n nearest neighbors or to examples within some distance \n\n\n\nϵ\n\n\n{\\displaystyle \\epsilon }\n\n. The weight \n\n\n\n\nW\n\ni\nj\n\n\n\n\n{\\displaystyle W_{ij}}\n\n of an edge between \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n and \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n is then set to \n\n\n\n\ne\n\n−\n‖\n\nx\n\ni\n\n\n−\n\nx\n\nj\n\n\n\n‖\n\n2\n\n\n\n/\n\n\nϵ\n\n2\n\n\n\n\n\n\n{\\displaystyle e^{-\\|x_{i}-x_{j}\\|^{2}/\\epsilon ^{2}}}\n\n.\n Within the framework of manifold regularization,[10][11] the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes\n where \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n is a reproducing kernel Hilbert space and \n\n\n\n\n\nM\n\n\n\n\n{\\displaystyle {\\mathcal {M}}}\n\n is the manifold on which the data lie. The regularization parameters \n\n\n\n\nλ\n\nA\n\n\n\n\n{\\displaystyle \\lambda _{A}}\n\n and \n\n\n\n\nλ\n\nI\n\n\n\n\n{\\displaystyle \\lambda _{I}}\n\n control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian \n\n\n\nL\n=\nD\n−\nW\n\n\n{\\displaystyle L=D-W}\n\n where \n\n\n\n\nD\n\ni\ni\n\n\n=\n\n∑\n\nj\n=\n1\n\n\nl\n+\nu\n\n\n\nW\n\ni\nj\n\n\n\n\n{\\displaystyle D_{ii}=\\sum _{j=1}^{l+u}W_{ij}}\n\n and \n\n\n\n\nf\n\n\n\n{\\displaystyle \\mathbf {f} }\n\n is the vector \n\n\n\n[\nf\n(\n\nx\n\n1\n\n\n)\n…\nf\n(\n\nx\n\nl\n+\nu\n\n\n)\n]\n\n\n{\\displaystyle [f(x_{1})\\dots f(x_{l+u})]}\n\n, we have\n The graph-based approach to Laplacian regularization is to put in relation with finite difference method.[clarification needed][citation needed]\n The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.\n Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples \n\n\n\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nl\n+\nu\n\n\n\n\n{\\displaystyle x_{1},\\dots ,x_{l+u}}\n\n may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation.[12][13] Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.\n Self-training is a wrapper method for semi-supervised learning.[14] First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.[15] In natural language processing, a common self-training algorithm is the Yarowsky algorithm for problems like word sense disambiguation, accent restoration, and spelling correction.[16]\n Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.[17]\n Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data.[18] More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).\n Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.[19] Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.[20][21]\n",
        "url": "https://en.wikipedia.org/wiki/Semi-supervised_learning"
    },
    {
        "id": 9,
        "title": "Self-supervised learning",
        "content": "\n\n Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]\n During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters.[2][3] Next, the actual task is performed with supervised or unsupervised learning.[4][5][6]\n Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition.[7]\n Autoassociative self-supervised learning is a specific category of self-supervised learning where a neural network is trained to reproduce or reconstruct its own input data.[8] In other words, the model is tasked with learning a representation of the data that captures its essential features or structure, allowing it to regenerate the original input.\n The term \"autoassociative\" comes from the fact that the model is essentially associating the input data with itself. This is often achieved using autoencoders, which are a type of neural network architecture used for representation learning. Autoencoders consist of an encoder network that maps the input data to a lower-dimensional representation (latent space), and a decoder network that reconstructs the input from this representation. \n The training process involves presenting the model with input data and requiring it to reconstruct the same data as closely as possible. The loss function used during training typically penalizes the difference between the original input and the reconstructed output (e.g. mean squared error). By minimizing this reconstruction error, the autoencoder learns a meaningful representation of the data in its latent space.\n For a binary classification task, training data can be divided into positive examples and negative examples. Positive examples are those that match the target. For example, if training a classifier to identify birds, the positive training data would include images that contain birds. Negative examples would be images that do not.[9] Contrastive self-supervised learning uses both positive and negative examples. The loss function in contrastive learning is used to minimize the distance between positive sample pairs, while maximizing the distance between negative sample pairs.[9]\n An early example uses a pair of 1-dimensional convolutional neural networks to process a pair of images and maximize their agreement.[10]\n Contrastive Language-Image Pre-training (CLIP) allows joint pretraining of a text encoder and an image encoder, such that a matching image-text pair have image encoding vector and text encoding vector that span a small angle (having a large cosine similarity).\n InfoNCE (Noise-Contrastive Estimation)[11] is a method to optimize two models jointly, based on Noise Contrastive Estimation (NCE).[12] Given a set \n\n\n\nX\n=\n\n{\n\n\nx\n\n1\n\n\n,\n…\n\nx\n\nN\n\n\n\n}\n\n\n\n{\\displaystyle X=\\left\\{x_{1},\\ldots x_{N}\\right\\}}\n\n of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n random samples containing one positive sample from \n\n\n\np\n\n(\n\n\nx\n\nt\n+\nk\n\n\n∣\n\nc\n\nt\n\n\n\n)\n\n\n\n{\\displaystyle p\\left(x_{t+k}\\mid c_{t}\\right)}\n\n and \n\n\n\nN\n−\n1\n\n\n{\\displaystyle N-1}\n\n negative samples from the 'proposal' distribution \n\n\n\np\n\n(\n\nx\n\nt\n+\nk\n\n\n)\n\n\n\n{\\displaystyle p\\left(x_{t+k}\\right)}\n\n, it minimizes the following loss function:\n \n\n\n\n\n\n\nL\n\n\n\n\nN\n\n\n\n=\n−\n\n\nE\n\n\nX\n\n\n\n[\n\nlog\n⁡\n\n\n\n\nf\n\nk\n\n\n\n(\n\n\nx\n\nt\n+\nk\n\n\n,\n\nc\n\nt\n\n\n\n)\n\n\n\n\n∑\n\n\nx\n\nj\n\n\n∈\nX\n\n\n\nf\n\nk\n\n\n\n(\n\n\nx\n\nj\n\n\n,\n\nc\n\nt\n\n\n\n)\n\n\n\n\n\n]\n\n\n\n{\\displaystyle {\\mathcal {L}}_{\\mathrm {N} }=-\\mathbb {E} _{X}\\left[\\log {\\frac {f_{k}\\left(x_{t+k},c_{t}\\right)}{\\sum _{x_{j}\\in X}f_{k}\\left(x_{j},c_{t}\\right)}}\\right]}\n\n\n Non-contrastive self-supervised learning (NCSSL) uses only positive examples. Counterintuitively, NCSSL converges on a useful local minimum rather than reaching a trivial solution, with zero loss. For the example of binary classification, it would trivially learn to classify each example as positive. Effective NCSSL requires an extra predictor on the online side that does not back-propagate on the target side.[9]\n SSL belongs to supervised learning methods insofar as the goal is to generate a classified output from the input. At the same time, however, it does not require the explicit use of labeled input-output pairs. Instead, correlations, metadata embedded in the data, or domain knowledge present in the input are implicitly and autonomously extracted from the data. These supervisory signals, extracted from the data, can then be used for training.[1]\n SSL is similar to unsupervised learning in that it does not require labels in the sample data. Unlike unsupervised learning, however, learning is not done using inherent data structures.\n Semi-supervised learning combines supervised and unsupervised learning, requiring only a small portion of the learning data be labeled.[3]\n In transfer learning, a model designed for one task is reused on a different task.[13]\n Training an autoencoder intrinsically constitutes a self-supervised process, because the output pattern needs to become an optimal reconstruction of the input pattern itself. However, in current jargon, the term 'self-supervised' often refers to tasks based on a pretext-task training setup. This involves the (human) design of such pretext task(s), unlike \nthe case of fully self-contained autoencoder training.[8]\n In reinforcement learning, self-supervising learning from a combination of losses can create abstract representations where only the most important information about the state are kept in a compressed way.[14]\n Self-supervised learning is particularly suitable for speech recognition. For example, Facebook developed wav2vec, a self-supervised algorithm, to perform speech recognition using two deep convolutional neural networks that build on each other.[7]\n Google's Bidirectional Encoder Representations from Transformers (BERT) model is used to better understand the context of search queries.[15]\n OpenAI's GPT-3 is an autoregressive language model that can be used in language processing. It can be used to translate texts or answer questions, among other things.[16]\n Bootstrap Your Own Latent (BYOL) is a NCSSL that produced excellent results on ImageNet and on transfer and semi-supervised benchmarks.[17]\n The Yarowsky algorithm is an example of self-supervised learning in natural language processing. From a small number of labeled examples, it learns to predict which word sense of a polysemous word is being used at a given point in text.\n DirectPred is a NCSSL that directly sets the predictor weights instead of learning it via typical gradient descent.[9]\n Self-GenomeNet is an example of self-supervised learning in genomics.[18]\n Self-supervised learning continues to gain prominence as a new approach across diverse fields. Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.\n",
        "url": "https://en.wikipedia.org/wiki/Self-supervised_learning"
    },
    {
        "id": 10,
        "title": "Reinforcement learning",
        "content": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. \n Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).[1] The search for this balance is known as the exploration-exploitation dilemma.\n \nThe environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques.[2] The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.[3]  Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).\n Basic reinforcement learning is modeled as a Markov decision process:\n The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]\n A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state \n\n\n\n\nS\n\nt\n\n\n\n\n{\\displaystyle S_{t}}\n\n and reward \n\n\n\n\nR\n\nt\n\n\n\n\n{\\displaystyle R_{t}}\n\n. It then chooses an action \n\n\n\n\nA\n\nt\n\n\n\n\n{\\displaystyle A_{t}}\n\n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n\n\n\n\nS\n\nt\n+\n1\n\n\n\n\n{\\displaystyle S_{t+1}}\n\n and the reward \n\n\n\n\nR\n\nt\n+\n1\n\n\n\n\n{\\displaystyle R_{t+1}}\n\n associated with the transition \n\n\n\n(\n\nS\n\nt\n\n\n,\n\nA\n\nt\n\n\n,\n\nS\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle (S_{t},A_{t},S_{t+1})}\n\n is determined. The goal of a reinforcement learning agent is to learn a policy: \n \n\n\n\nπ\n:\n\n\nS\n\n\n×\n\n\nA\n\n\n→\n[\n0\n,\n1\n]\n\n\n{\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}\n\n, \n\n\n\nπ\n(\ns\n,\na\n)\n=\nPr\n(\n\nA\n\nt\n\n\n=\na\n∣\n\nS\n\nt\n\n\n=\ns\n)\n\n\n{\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}\n\n\n that maximizes the expected cumulative reward.\n Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\n When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.\n Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage,[6] robot control,[7] photovoltaic generators,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]\n Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:\n The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\n The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]\n Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\n One such method is \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n-greedy, where \n\n\n\n0\n<\nε\n<\n1\n\n\n{\\displaystyle 0<\\varepsilon <1}\n\n is a parameter controlling the amount of exploration vs. exploitation.  With probability \n\n\n\n1\n−\nε\n\n\n{\\displaystyle 1-\\varepsilon }\n\n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n, exploration is chosen, and the action is chosen uniformly at random. \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]\n Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\n The agent's action selection is modeled as a map called policy:\n The policy map gives the probability of taking action \n\n\n\na\n\n\n{\\displaystyle a}\n\n when in state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n.[14]: 61  There are also deterministic policies.\n The state-value function \n\n\n\n\nV\n\nπ\n\n\n(\ns\n)\n\n\n{\\displaystyle V_{\\pi }(s)}\n\n is defined as, expected discounted return starting with state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n, i.e. \n\n\n\n\nS\n\n0\n\n\n=\ns\n\n\n{\\displaystyle S_{0}=s}\n\n, and successively following policy \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]: 60 \n where the random variable \n\n\n\nG\n\n\n{\\displaystyle G}\n\n denotes the discounted return, and is defined as the sum of future discounted rewards:\n where \n\n\n\n\nR\n\nt\n+\n1\n\n\n\n\n{\\displaystyle R_{t+1}}\n\n is the reward for transitioning from state \n\n\n\n\nS\n\nt\n\n\n\n\n{\\displaystyle S_{t}}\n\n to \n\n\n\n\nS\n\nt\n+\n1\n\n\n\n\n{\\displaystyle S_{t+1}}\n\n, \n\n\n\n0\n≤\nγ\n<\n1\n\n\n{\\displaystyle 0\\leq \\gamma <1}\n\n is the discount rate. \n\n\n\nγ\n\n\n{\\displaystyle \\gamma }\n\n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\n The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\n The brute force approach entails two steps:\n One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\n These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\n Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n\n\n\n\n\nE\n\n\n⁡\n[\nG\n]\n\n\n{\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n\n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\n These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\n To define optimality in a formal manner, define the state-value of a policy \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n by\n where \n\n\n\nG\n\n\n{\\displaystyle G}\n\n stands for the discounted return associated with following \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n from the initial state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n. Defining \n\n\n\n\nV\n\n∗\n\n\n(\ns\n)\n\n\n{\\displaystyle V^{*}(s)}\n\n as the maximum possible state-value of \n\n\n\n\nV\n\nπ\n\n\n(\ns\n)\n\n\n{\\displaystyle V^{\\pi }(s)}\n\n, where \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n is allowed to change,\n A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since \n\n\n\n\nV\n\n∗\n\n\n(\ns\n)\n=\n\nmax\n\nπ\n\n\n\nE\n\n[\nG\n∣\ns\n,\nπ\n]\n\n\n{\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}\n\n, where \n\n\n\ns\n\n\n{\\displaystyle s}\n\n is a state randomly sampled from the distribution \n\n\n\nμ\n\n\n{\\displaystyle \\mu }\n\n of initial states (so \n\n\n\nμ\n(\ns\n)\n=\nPr\n(\n\nS\n\n0\n\n\n=\ns\n)\n\n\n{\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n\n).\n Although state-values suffice to define optimality, it is useful to define action-values. Given a state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n, an action \n\n\n\na\n\n\n{\\displaystyle a}\n\n and a policy \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n, the action-value of the pair \n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle (s,a)}\n\n under \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n is defined by\n where \n\n\n\nG\n\n\n{\\displaystyle G}\n\n now stands for the random discounted return associated with first taking action \n\n\n\na\n\n\n{\\displaystyle a}\n\n in state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n and following \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n, thereafter.\n The theory of Markov decision processes states that if \n\n\n\n\nπ\n\n∗\n\n\n\n\n{\\displaystyle \\pi ^{*}}\n\n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n\n\n\n\nQ\n\n\nπ\n\n∗\n\n\n\n\n(\ns\n,\n⋅\n)\n\n\n{\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n\n with the highest action-value at each state, \n\n\n\ns\n\n\n{\\displaystyle s}\n\n. The action-value function of such an optimal policy (\n\n\n\n\nQ\n\n\nπ\n\n∗\n\n\n\n\n\n\n{\\displaystyle Q^{\\pi ^{*}}}\n\n) is called the optimal action-value function and is commonly denoted by \n\n\n\n\nQ\n\n∗\n\n\n\n\n{\\displaystyle Q^{*}}\n\n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\n Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n\n\n\n\nQ\n\nk\n\n\n\n\n{\\displaystyle Q_{k}}\n\n (\n\n\n\nk\n=\n0\n,\n1\n,\n2\n,\n…\n\n\n{\\displaystyle k=0,1,2,\\ldots }\n\n) that converge to \n\n\n\n\nQ\n\n∗\n\n\n\n\n{\\displaystyle Q^{*}}\n\n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\n Monte Carlo methods[15] are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment’s dynamics, Monte Carlo methods rely solely on actual or simulated experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.\n Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term \"Monte Carlo\" generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.\n These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.[14]\n The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\n The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[16][17] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[18] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\n Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n parameter \n\n\n\n(\n0\n≤\nλ\n≤\n1\n)\n\n\n{\\displaystyle (0\\leq \\lambda \\leq 1)}\n\n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\n In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle (s,a)}\n\n are obtained by linearly combining the components of \n\n\n\nϕ\n(\ns\n,\na\n)\n\n\n{\\displaystyle \\phi (s,a)}\n\n with some weights \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n:\n The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\n Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[19] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]\n The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\n An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\n Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n, let \n\n\n\n\nπ\n\nθ\n\n\n\n\n{\\displaystyle \\pi _{\\theta }}\n\n denote the policy associated to \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n. Defining the performance function by \n\n\n\nρ\n(\nθ\n)\n=\n\nρ\n\n\nπ\n\nθ\n\n\n\n\n\n\n{\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n\n under mild conditions this function will be differentiable as a function of the parameter vector \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n. If the gradient of \n\n\n\nρ\n\n\n{\\displaystyle \\rho }\n\n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22]\n A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\n Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[23]\n Policy search methods have been used in the robotics context.[24] Many policy search methods may get stuck in local optima (as they are based on local search).\n Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26] to the learning algorithm.\n Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]\n There are other ways to use models than to update a value function.[28] For instance, in model predictive control the model is used to update the behavior directly.\n Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\n Efficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\n For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\n Research topics include:\n Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]\n This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[47] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[48]\n Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[49][50][51] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[52]\n By introducing fuzzy inference in reinforcement learning,[53] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [54] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\n In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[55] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [56] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [57] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\n Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[58] An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the Conditional Value at Risk (CVaR).[59] In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61] However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62] and blindness to success.[63]\n Self-reinforcement learning (or self learning), is a learning paradigm which does not use the concept of immediate reward Ra(s,s') after transition from s to s' with action a. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation. \n The self-reinforcement algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine:\n1. in situation s perform action a\n2. receive a consequence situation s'\n3. compute state evaluation v(s') of how good is to be in the consequence situation s'\n4. update crossbar memory w'(a,s) = w(a,s) + v(s')\n Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior). \n Self reinforcement (self learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA).[64][65] The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion. [66]\n Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[67] After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test.[68] This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[69]\n",
        "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
        "id": 11,
        "title": "Meta-learning (computer science)",
        "content": "Meta-learning[1][2]\nis a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\n Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias.[3] This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.\n By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta-learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987)[1] and Yoshua Bengio et al.'s work (1991),[4] considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta-learning system[1] using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.[1]\n A proposed definition[5] for a meta-learning system combines three requirements:\n Bias refers to the assumptions that influence the choice of explanatory hypotheses[6] and not the notion of bias represented in the bias-variance dilemma. Meta-learning is concerned with two aspects of learning bias.\n There are three common approaches:[8]\n Model-based meta-learning models updates its parameters rapidly with a few training steps, which can be achieved by its internal architecture or controlled by another meta-learner model.[8]\n A Memory-Augmented Neural Network, or MANN for short, is claimed to be able to encode new information quickly and thus to adapt to new tasks after only a few examples.[9]\n Meta Networks (MetaNet) learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization.[10]\n The core idea in metric-based meta-learning is similar to nearest neighbors algorithms, which weight is generated by a kernel function. It aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving.[8]\n Siamese neural network is composed of two twin networks whose output is jointly trained. There is a function above to learn the relationship between input data sample pairs. The two networks are the same, sharing the same weight and network parameters.[11]\n Matching Networks learn a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types.[12]\n The Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting.[13]\n Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve satisfied results.[14]\n What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples.[8]\n LSTM-based meta-learner is to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training.[15]\n Model-Agnostic Meta-Learning (MAML) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.[16]\n Reptile is a remarkably simple meta-learning optimization algorithm, given that both of its components rely on meta-optimization through gradient descent and both are model-agnostic.[17]\n Some approaches which have been viewed as instances of meta-learning:\n",
        "url": "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)"
    },
    {
        "id": 12,
        "title": "Online machine learning",
        "content": "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., prediction of prices in the financial international markets. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n In the setting of supervised learning, a function of \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is to be learned, where \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is thought of as a space of inputs and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n on \n\n\n\nX\n×\nY\n\n\n{\\displaystyle X\\times Y}\n\n. In reality, the learner never knows the true distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n over instances. Instead, the learner usually has access to a training set of examples \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n…\n,\n(\n\nx\n\nn\n\n\n,\n\ny\n\nn\n\n\n)\n\n\n{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{n},y_{n})}\n\n. In this setting, the loss function is given as \n\n\n\nV\n:\nY\n×\nY\n→\n\nR\n\n\n\n{\\displaystyle V:Y\\times Y\\to \\mathbb {R} }\n\n, such that \n\n\n\nV\n(\nf\n(\nx\n)\n,\ny\n)\n\n\n{\\displaystyle V(f(x),y)}\n\n measures the difference between the predicted value \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n and the true value \n\n\n\ny\n\n\n{\\displaystyle y}\n\n. The ideal goal is to select a function \n\n\n\nf\n∈\n\n\nH\n\n\n\n\n{\\displaystyle f\\in {\\mathcal {H}}}\n\n, where \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\n In statistical learning models, the training sample \n\n\n\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},y_{i})}\n\n are assumed to have been drawn from the true distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n and the objective is to minimize the expected \"risk\"\n\n\n\n\nI\n[\nf\n]\n=\n\nE\n\n[\nV\n(\nf\n(\nx\n)\n,\ny\n)\n]\n=\n∫\nV\n(\nf\n(\nx\n)\n,\ny\n)\n\nd\np\n(\nx\n,\ny\n)\n \n.\n\n\n{\\displaystyle I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ .}\n\n\nA common paradigm in this situation is to estimate a function \n\n\n\n\n\n\nf\n^\n\n\n\n\n\n{\\displaystyle {\\hat {f}}}\n\n through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.\nA purely online model in this category would learn based on just the new input \n\n\n\n(\n\nx\n\nt\n+\n1\n\n\n,\n\ny\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle (x_{t+1},y_{t+1})}\n\n, the current best predictor \n\n\n\n\nf\n\nt\n\n\n\n\n{\\displaystyle f_{t}}\n\n and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where \n\n\n\n\nf\n\nt\n+\n1\n\n\n\n\n{\\displaystyle f_{t+1}}\n\n is permitted to depend on \n\n\n\n\nf\n\nt\n\n\n\n\n{\\displaystyle f_{t}}\n\n and all previous data points \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n…\n,\n(\n\nx\n\nt\n\n\n,\n\ny\n\nt\n\n\n)\n\n\n{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{t},y_{t})}\n\n. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\n A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of \n\n\n\nb\n≥\n1\n\n\n{\\displaystyle b\\geq 1}\n\n data points at a time, this can be considered as pseudo-online learning for \n\n\n\nb\n\n\n{\\displaystyle b}\n\n much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.\n The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.\n Consider the setting of supervised learning with \n\n\n\nf\n\n\n{\\displaystyle f}\n\n being a linear function to be learned:\n\n\n\n\nf\n(\n\nx\n\nj\n\n\n)\n=\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n=\nw\n⋅\n\nx\n\nj\n\n\n\n\n{\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}\n\n\nwhere \n\n\n\n\nx\n\nj\n\n\n∈\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}\n\n is a vector of inputs (data points) and \n\n\n\nw\n∈\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle w\\in \\mathbb {R} ^{d}}\n\n is a linear filter vector.\nThe goal is to compute the filter vector \n\n\n\nw\n\n\n{\\displaystyle w}\n\n.\nTo this end, a square loss function \n\n\n\n\nV\n(\nf\n(\n\nx\n\nj\n\n\n)\n,\n\ny\n\nj\n\n\n)\n=\n(\nf\n(\n\nx\n\nj\n\n\n)\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n=\n(\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}\n\n\nis used to compute the vector \n\n\n\nw\n\n\n{\\displaystyle w}\n\n that minimizes the empirical loss\n\n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n=\n\n∑\n\nj\n=\n1\n\n\nn\n\n\nV\n(\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n,\n\ny\n\nj\n\n\n)\n=\n\n∑\n\nj\n=\n1\n\n\nn\n\n\n(\n\nx\n\nj\n\n\n\nT\n\n\n\nw\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{\\mathsf {T}}w-y_{j})^{2}}\n\n \nwhere\n\n\n\n\n\ny\n\nj\n\n\n∈\n\nR\n\n.\n\n\n{\\displaystyle y_{j}\\in \\mathbb {R} .}\n\n\n Let \n\n\n\nX\n\n\n{\\displaystyle X}\n\n be the \n\n\n\ni\n×\nd\n\n\n{\\displaystyle i\\times d}\n\n data matrix and \n\n\n\ny\n∈\n\n\nR\n\n\ni\n\n\n\n\n{\\displaystyle y\\in \\mathbb {R} ^{i}}\n\n is the column vector of target values after the arrival of the first \n\n\n\ni\n\n\n{\\displaystyle i}\n\n data points.\nAssuming that the covariance matrix \n\n\n\n\nΣ\n\ni\n\n\n=\n\nX\n\n\nT\n\n\n\nX\n\n\n{\\displaystyle \\Sigma _{i}=X^{\\mathsf {T}}X}\n\n is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution \n\n\n\n\nf\n\n∗\n\n\n(\nx\n)\n=\n⟨\n\nw\n\n∗\n\n\n,\nx\n⟩\n\n\n{\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }\n\n to the linear least squares problem is given by\n\n\n\n\n\nw\n\n∗\n\n\n=\n(\n\nX\n\n\nT\n\n\n\nX\n\n)\n\n−\n1\n\n\n\nX\n\n\nT\n\n\n\ny\n=\n\nΣ\n\ni\n\n\n−\n1\n\n\n\n∑\n\nj\n=\n1\n\n\ni\n\n\n\nx\n\nj\n\n\n\ny\n\nj\n\n\n.\n\n\n{\\displaystyle w^{*}=(X^{\\mathsf {T}}X)^{-1}X^{\\mathsf {T}}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}.}\n\n\n Now, calculating the covariance matrix \n\n\n\n\nΣ\n\ni\n\n\n=\n\n∑\n\nj\n=\n1\n\n\ni\n\n\n\nx\n\nj\n\n\n\nx\n\nj\n\n\n\nT\n\n\n\n\n\n{\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{\\mathsf {T}}}\n\n takes time \n\n\n\nO\n(\ni\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(id^{2})}\n\n, inverting the \n\n\n\nd\n×\nd\n\n\n{\\displaystyle d\\times d}\n\n matrix takes time \n\n\n\nO\n(\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(d^{3})}\n\n, while the rest of the multiplication takes time \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n, giving a total time of \n\n\n\nO\n(\ni\n\nd\n\n2\n\n\n+\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(id^{2}+d^{3})}\n\n. When there are \n\n\n\nn\n\n\n{\\displaystyle n}\n\n total points in the dataset, to recompute the solution after the arrival of every datapoint \n\n\n\ni\n=\n1\n,\n…\n,\nn\n\n\n{\\displaystyle i=1,\\ldots ,n}\n\n, the naive approach will have a total complexity \n\n\n\nO\n(\n\nn\n\n2\n\n\n\nd\n\n2\n\n\n+\nn\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(n^{2}d^{2}+nd^{3})}\n\n. Note that when storing the matrix \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n, then updating it at each step needs only adding \n\n\n\n\nx\n\ni\n+\n1\n\n\n\nx\n\ni\n+\n1\n\n\n\nT\n\n\n\n\n\n{\\displaystyle x_{i+1}x_{i+1}^{\\mathsf {T}}}\n\n, which takes \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n time, reducing the total time to \n\n\n\nO\n(\nn\n\nd\n\n2\n\n\n+\nn\n\nd\n\n3\n\n\n)\n=\nO\n(\nn\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}\n\n, but with an additional storage space of \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n to store \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n.[1]\n The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising \n\n\n\n\n\nw\n\n0\n\n\n=\n0\n∈\n\n\nR\n\n\nd\n\n\n\n\n\n{\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}\n\n and \n\n\n\n\n\nΓ\n\n0\n\n\n=\nI\n∈\n\n\nR\n\n\nd\n×\nd\n\n\n\n\n\n{\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}\n\n, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:\n\n\n\n\n\nΓ\n\ni\n\n\n=\n\nΓ\n\ni\n−\n1\n\n\n−\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\nx\n\ni\n\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\n\n1\n+\n\nx\n\ni\n\n\n\nT\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\nx\n\ni\n\n\n\n\n\n\n\n{\\displaystyle \\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{\\mathsf {T}}\\Gamma _{i-1}}{1+x_{i}^{\\mathsf {T}}\\Gamma _{i-1}x_{i}}}}\n\n\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nΓ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)}\n\n\nThe above iteration algorithm can be proved using induction on \n\n\n\ni\n\n\n{\\displaystyle i}\n\n.[2] The proof also shows that \n\n\n\n\nΓ\n\ni\n\n\n=\n\nΣ\n\ni\n\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}\n\n. One can look at RLS also in the context of adaptive filters (see RLS).\n The complexity for \n\n\n\nn\n\n\n{\\displaystyle n}\n\n steps of this algorithm is \n\n\n\nO\n(\nn\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(nd^{2})}\n\n, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step \n\n\n\ni\n\n\n{\\displaystyle i}\n\n here are to store the matrix \n\n\n\n\nΓ\n\ni\n\n\n\n\n{\\displaystyle \\Gamma _{i}}\n\n, which is constant at \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n. For the case when \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n is not invertible, consider the regularised version of the problem loss function \n\n\n\n\n∑\n\nj\n=\n1\n\n\nn\n\n\n\n\n(\n\n\nx\n\nj\n\n\n\nT\n\n\n\nw\n−\n\ny\n\nj\n\n\n\n)\n\n\n2\n\n\n+\nλ\n\n\n‖\nw\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle \\sum _{j=1}^{n}\\left(x_{j}^{\\mathsf {T}}w-y_{j}\\right)^{2}+\\lambda \\left\\|w\\right\\|_{2}^{2}}\n\n. Then, it's easy to show that the same algorithm works with \n\n\n\n\nΓ\n\n0\n\n\n=\n(\nI\n+\nλ\nI\n\n)\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}\n\n, and the iterations proceed to give \n\n\n\n\nΓ\n\ni\n\n\n=\n(\n\nΣ\n\ni\n\n\n+\nλ\nI\n\n)\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}\n\n.[1]\n When this \n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nΓ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)}\n\n \nis replaced by\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n∇\nV\n(\n⟨\n\nw\n\ni\n−\n1\n\n\n,\n\nx\n\ni\n\n\n⟩\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}\n\n \nor \n\n\n\n\nΓ\n\ni\n\n\n∈\n\n\nR\n\n\nd\n×\nd\n\n\n\n\n{\\displaystyle \\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}}\n\n by \n\n\n\n\nγ\n\ni\n\n\n∈\n\nR\n\n\n\n{\\displaystyle \\gamma _{i}\\in \\mathbb {R} }\n\n, this becomes the stochastic gradient descent algorithm. In this case, the complexity for \n\n\n\nn\n\n\n{\\displaystyle n}\n\n steps of this algorithm reduces to \n\n\n\nO\n(\nn\nd\n)\n\n\n{\\displaystyle O(nd)}\n\n. The storage requirements at every step \n\n\n\ni\n\n\n{\\displaystyle i}\n\n are constant at \n\n\n\nO\n(\nd\n)\n\n\n{\\displaystyle O(d)}\n\n.\n However, the stepsize \n\n\n\n\nγ\n\ni\n\n\n\n\n{\\displaystyle \\gamma _{i}}\n\n needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size \n\n\n\n\nγ\n\ni\n\n\n≈\n\n\n1\n\ni\n\n\n\n,\n\n\n{\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}\n\n one can prove the convergence of the average iterate \n\n\n\n\n\n\nw\n¯\n\n\n\nn\n\n\n=\n\n\n1\nn\n\n\n\n∑\n\ni\n=\n1\n\n\nn\n\n\n\nw\n\ni\n\n\n\n\n{\\textstyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}\n\n. This setting is a special case of stochastic optimization, a well known problem in optimization.[1]\n In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is called incremental gradient method and corresponds to an iteration\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n∇\nV\n(\n⟨\n\nw\n\ni\n−\n1\n\n\n,\n\nx\n\n\nt\n\ni\n\n\n\n\n⟩\n,\n\ny\n\n\nt\n\ni\n\n\n\n\n)\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}\n\n \nThe main difference with the stochastic gradient method is that here a sequence \n\n\n\n\nt\n\ni\n\n\n\n\n{\\displaystyle t_{i}}\n\n is chosen to decide which training point is visited in the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk.[3] Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.[1]\n Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method. This discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction [1] that if \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n is the data matrix and \n\n\n\n\nw\n\ni\n\n\n\n\n{\\displaystyle w_{i}}\n\n is the output after \n\n\n\ni\n\n\n{\\displaystyle i}\n\n steps of the SGD algorithm, then,\n\n\n\n\n\nw\n\ni\n\n\n=\n\nX\n\ni\n\n\n\nT\n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle w_{i}=X_{i}^{\\mathsf {T}}c_{i}}\n\n \nwhere \n\n\n\n\nc\n\ni\n\n\n=\n(\n(\n\nc\n\ni\n\n\n\n)\n\n1\n\n\n,\n(\n\nc\n\ni\n\n\n\n)\n\n2\n\n\n,\n.\n.\n.\n,\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n)\n∈\n\n\nR\n\n\ni\n\n\n\n\n{\\displaystyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}\n\n and the sequence \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n satisfies the recursion:\n\n\n\n\n\nc\n\n0\n\n\n=\n0\n\n\n{\\displaystyle c_{0}=0}\n\n\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\nj\n\n\n=\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n,\nj\n=\n1\n,\n2\n,\n.\n.\n.\n,\ni\n−\n1\n\n\n{\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}\n\n and\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n=\n\nγ\n\ni\n\n\n\n\n(\n\n\n\ny\n\ni\n\n\n−\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n⟨\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n⟩\n\n\n)\n\n\n\n\n{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}\n\n\nNotice that here \n\n\n\n⟨\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n⟩\n\n\n{\\displaystyle \\langle x_{j},x_{i}\\rangle }\n\n is just the standard Kernel on \n\n\n\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{d}}\n\n, and the predictor is of the form \n\n\n\n\n\nf\n\ni\n\n\n(\nx\n)\n=\n⟨\n\nw\n\ni\n−\n1\n\n\n,\nx\n⟩\n=\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n⟨\n\nx\n\nj\n\n\n,\nx\n⟩\n.\n\n\n{\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle .}\n\n\n \nNow, if a general kernel \n\n\n\nK\n\n\n{\\displaystyle K}\n\n is introduced instead and let the predictor be \n\n\n\n\n\nf\n\ni\n\n\n(\nx\n)\n=\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\nK\n(\n\nx\n\nj\n\n\n,\nx\n)\n\n\n{\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}\n\n\nthen the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n=\n\nγ\n\ni\n\n\n\n\n(\n\n\n\ny\n\ni\n\n\n−\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\nK\n(\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n)\n\n\n)\n\n\n\n\n{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}\n\n\nThe above expression requires storing all the data for updating \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n. The total time complexity for the recursion when evaluating for the \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-th datapoint is \n\n\n\nO\n(\n\nn\n\n2\n\n\nd\nk\n)\n\n\n{\\displaystyle O(n^{2}dk)}\n\n, where \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is the cost of evaluating the kernel on a single pair of points.[1] Thus, the use of the kernel has allowed the movement from a finite dimensional parameter space \n\n\n\n\n\nw\n\ni\n\n\n∈\n\n\nR\n\n\nd\n\n\n\n\n\n{\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}\n\n to a possibly infinite dimensional feature represented by a kernel \n\n\n\nK\n\n\n{\\displaystyle K}\n\n by instead performing the recursion on the space of parameters \n\n\n\n\n\nc\n\ni\n\n\n∈\n\n\nR\n\n\ni\n\n\n\n\n\n{\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}\n\n, whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.[1]\n Online convex optimization (OCO) [4] is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:\n For \n\n\n\nt\n=\n1\n,\n2\n,\n.\n.\n.\n,\nT\n\n\n{\\displaystyle t=1,2,...,T}\n\n\n The goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point \n\n\n\nu\n∈\nS\n\n\n{\\displaystyle u\\in S}\n\n in hindsight. As an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set \n\n\n\nS\n=\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle S=\\mathbb {R} ^{d}}\n\n, and nature sends back the convex loss function \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n(\n⟨\nw\n,\n\nx\n\nt\n\n\n⟩\n−\n\ny\n\nt\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}\n\n. Note here that \n\n\n\n\ny\n\nt\n\n\n\n\n{\\displaystyle y_{t}}\n\n is implicitly sent with \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n.\n Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.[citation needed]\n Some simple online convex optimisation algorithms are:\n The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and round \n\n\n\nt\n\n\n{\\displaystyle t}\n\n is simply given by:\n\n\n\n\n\nw\n\nt\n\n\n=\n\n\n\na\nr\ng\n\nm\ni\nn\n\n\n\nw\n∈\nS\n\n\n⁡\n\n∑\n\ni\n=\n1\n\n\nt\n−\n1\n\n\n\nv\n\ni\n\n\n(\nw\n)\n\n\n{\\displaystyle w_{t}=\\mathop {\\operatorname {arg\\,min} } _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}\n\n\nThis method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n\n\n‖\n\nw\n−\n\nx\n\nt\n\n\n\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle v_{t}(w)=\\left\\|w-x_{t}\\right\\|_{2}^{2}}\n\n), one can show a regret bound that grows as \n\n\n\nlog\n⁡\n(\nT\n)\n\n\n{\\displaystyle \\log(T)}\n\n. However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.\n This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function \n\n\n\nR\n:\nS\n→\n\nR\n\n\n\n{\\displaystyle R:S\\to \\mathbb {R} }\n\n is chosen and learning performed in round t as follows:\n\n\n\n\n\nw\n\nt\n\n\n=\n\n\n\na\nr\ng\n\nm\ni\nn\n\n\n\nw\n∈\nS\n\n\n⁡\n\n∑\n\ni\n=\n1\n\n\nt\n−\n1\n\n\n\nv\n\ni\n\n\n(\nw\n)\n+\nR\n(\nw\n)\n\n\n{\\displaystyle w_{t}=\\mathop {\\operatorname {arg\\,min} } _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}\n\n\nAs a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n⟨\nw\n,\n\nz\n\nt\n\n\n⟩\n\n\n{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n\n. Also, let \n\n\n\nS\n=\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle S=\\mathbb {R} ^{d}}\n\n. Suppose the regularisation function \n\n\n\nR\n(\nw\n)\n=\n\n\n1\n\n2\nη\n\n\n\n\n\n‖\nw\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\textstyle R(w)={\\frac {1}{2\\eta }}\\left\\|w\\right\\|_{2}^{2}}\n\n is chosen for some positive number \n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n. Then, one can show that the regret minimising iteration becomes \n\n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n−\nη\n\n∑\n\ni\n=\n1\n\n\nt\n\n\n\nz\n\ni\n\n\n=\n\nw\n\nt\n\n\n−\nη\n\nz\n\nt\n\n\n\n\n{\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}\n\n\nNote that this can be rewritten as \n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nw\n\nt\n\n\n−\nη\n∇\n\nv\n\nt\n\n\n(\n\nw\n\nt\n\n\n)\n\n\n{\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}\n\n, which looks exactly like online gradient descent.\n If S is instead some convex subspace of \n\n\n\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{d}}\n\n, S would need to be projected onto, leading to the modified update rule\n\n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nΠ\n\nS\n\n\n(\n−\nη\n\n∑\n\ni\n=\n1\n\n\nt\n\n\n\nz\n\ni\n\n\n)\n=\n\nΠ\n\nS\n\n\n(\nη\n\nθ\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}\n\n\nThis algorithm is known as lazy projection, as the vector \n\n\n\n\nθ\n\nt\n+\n1\n\n\n\n\n{\\displaystyle \\theta _{t+1}}\n\n accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n, and thus the average regret goes to 0 as desired.\n The above proved a regret bound for linear loss functions \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n⟨\nw\n,\n\nz\n\nt\n\n\n⟩\n\n\n{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n\n. To generalise the algorithm to any convex loss function, the subgradient \n\n\n\n∂\n\nv\n\nt\n\n\n(\n\nw\n\nt\n\n\n)\n\n\n{\\displaystyle \\partial v_{t}(w_{t})}\n\n of \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n is used as a linear approximation to \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n near \n\n\n\n\nw\n\nt\n\n\n\n\n{\\displaystyle w_{t}}\n\n, leading to the online subgradient descent algorithm:\n Initialise parameter \n\n\n\nη\n,\n\nw\n\n1\n\n\n=\n0\n\n\n{\\displaystyle \\eta ,w_{1}=0}\n\n\n For \n\n\n\nt\n=\n1\n,\n2\n,\n.\n.\n.\n,\nT\n\n\n{\\displaystyle t=1,2,...,T}\n\n\n One can use the OSD algorithm to derive \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n regret bounds for the online version of SVM's for classification, which use the hinge loss\n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\nmax\n{\n0\n,\n1\n−\n\ny\n\nt\n\n\n(\nw\n⋅\n\nx\n\nt\n\n\n)\n}\n\n\n{\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}\n\n\n Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent. The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm. For the Euclidean regularisation, one can show a regret bound of \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n, which can be improved further to a \n\n\n\nO\n(\nlog\n⁡\nT\n)\n\n\n{\\displaystyle O(\\log T)}\n\n for strongly convex and exp-concave loss functions.\n Continual learning means constantly improving the learned model by processing continuous streams of information.[5] Continual learning capabilities are essential for software systems and autonomous agents interacting in an ever changing real world. However, continual learning is a challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting. \n The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions \n\n\n\n\nf\n\n1\n\n\n,\n\nf\n\n2\n\n\n,\n…\n,\n\nf\n\nn\n\n\n\n\n{\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}\n\n. The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by\n\n\n\n\n\nw\n\nt\n\n\n=\n\nw\n\nt\n−\n1\n\n\n−\n\nγ\n\nt\n\n\n∇\nV\n(\n⟨\n\nw\n\nt\n−\n1\n\n\n,\n\nx\n\nt\n\n\n⟩\n,\n\ny\n\nt\n\n\n)\n\n\n{\\displaystyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}\n\n\n The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n defined above.[6] Indeed, in the case of an infinite stream of data, since the examples \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n(\n\nx\n\n2\n\n\n,\n\ny\n\n2\n\n\n)\n,\n…\n\n\n{\\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\\ldots }\n\n are assumed to be drawn i.i.d. from the distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n, the sequence of gradients of \n\n\n\nV\n(\n⋅\n,\n⋅\n)\n\n\n{\\displaystyle V(\\cdot ,\\cdot )}\n\n in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation \n\n\n\nI\n[\n\nw\n\nt\n\n\n]\n−\nI\n[\n\nw\n\n∗\n\n\n]\n\n\n{\\displaystyle I[w_{t}]-I[w^{\\ast }]}\n\n, where \n\n\n\n\nw\n\n∗\n\n\n\n\n{\\displaystyle w^{\\ast }}\n\n is the minimizer of \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n.[7] This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.\n The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method.[3] In this case, one instead looks at the empirical risk:\n\n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n=\n\n\n1\nn\n\n\n\n∑\n\ni\n=\n1\n\n\nn\n\n\nV\n(\n⟨\nw\n,\n\nx\n\ni\n\n\n⟩\n,\n\ny\n\ni\n\n\n)\n \n.\n\n\n{\\displaystyle I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ .}\n\n\nSince the gradients of \n\n\n\nV\n(\n⋅\n,\n⋅\n)\n\n\n{\\displaystyle V(\\cdot ,\\cdot )}\n\n in the incremental gradient descent iterations are also stochastic estimates of the gradient of \n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n\n\n{\\displaystyle I_{n}[w]}\n\n, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations \n\n\n\n\nI\n\nn\n\n\n[\n\nw\n\nt\n\n\n]\n−\n\nI\n\nn\n\n\n[\n\nw\n\nn\n\n\n∗\n\n\n]\n\n\n{\\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]}\n\n, where \n\n\n\n\nw\n\nn\n\n\n∗\n\n\n\n\n{\\displaystyle w_{n}^{\\ast }}\n\n is the minimizer of \n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n\n\n{\\displaystyle I_{n}[w]}\n\n.\n Learning paradigms\n General algorithms\n Learning models\n",
        "url": "https://en.wikipedia.org/wiki/Online_machine_learning"
    },
    {
        "id": 13,
        "title": "Online machine learning",
        "content": "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., prediction of prices in the financial international markets. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n In the setting of supervised learning, a function of \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is to be learned, where \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is thought of as a space of inputs and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n on \n\n\n\nX\n×\nY\n\n\n{\\displaystyle X\\times Y}\n\n. In reality, the learner never knows the true distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n over instances. Instead, the learner usually has access to a training set of examples \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n…\n,\n(\n\nx\n\nn\n\n\n,\n\ny\n\nn\n\n\n)\n\n\n{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{n},y_{n})}\n\n. In this setting, the loss function is given as \n\n\n\nV\n:\nY\n×\nY\n→\n\nR\n\n\n\n{\\displaystyle V:Y\\times Y\\to \\mathbb {R} }\n\n, such that \n\n\n\nV\n(\nf\n(\nx\n)\n,\ny\n)\n\n\n{\\displaystyle V(f(x),y)}\n\n measures the difference between the predicted value \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n and the true value \n\n\n\ny\n\n\n{\\displaystyle y}\n\n. The ideal goal is to select a function \n\n\n\nf\n∈\n\n\nH\n\n\n\n\n{\\displaystyle f\\in {\\mathcal {H}}}\n\n, where \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n is a space of functions called a hypothesis space, so that some notion of total loss is minimized. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.\n In statistical learning models, the training sample \n\n\n\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},y_{i})}\n\n are assumed to have been drawn from the true distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n and the objective is to minimize the expected \"risk\"\n\n\n\n\nI\n[\nf\n]\n=\n\nE\n\n[\nV\n(\nf\n(\nx\n)\n,\ny\n)\n]\n=\n∫\nV\n(\nf\n(\nx\n)\n,\ny\n)\n\nd\np\n(\nx\n,\ny\n)\n \n.\n\n\n{\\displaystyle I[f]=\\mathbb {E} [V(f(x),y)]=\\int V(f(x),y)\\,dp(x,y)\\ .}\n\n\nA common paradigm in this situation is to estimate a function \n\n\n\n\n\n\nf\n^\n\n\n\n\n\n{\\displaystyle {\\hat {f}}}\n\n through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.\nA purely online model in this category would learn based on just the new input \n\n\n\n(\n\nx\n\nt\n+\n1\n\n\n,\n\ny\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle (x_{t+1},y_{t+1})}\n\n, the current best predictor \n\n\n\n\nf\n\nt\n\n\n\n\n{\\displaystyle f_{t}}\n\n and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where \n\n\n\n\nf\n\nt\n+\n1\n\n\n\n\n{\\displaystyle f_{t+1}}\n\n is permitted to depend on \n\n\n\n\nf\n\nt\n\n\n\n\n{\\displaystyle f_{t}}\n\n and all previous data points \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n…\n,\n(\n\nx\n\nt\n\n\n,\n\ny\n\nt\n\n\n)\n\n\n{\\displaystyle (x_{1},y_{1}),\\ldots ,(x_{t},y_{t})}\n\n. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\n A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of \n\n\n\nb\n≥\n1\n\n\n{\\displaystyle b\\geq 1}\n\n data points at a time, this can be considered as pseudo-online learning for \n\n\n\nb\n\n\n{\\displaystyle b}\n\n much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.\n The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.\n Consider the setting of supervised learning with \n\n\n\nf\n\n\n{\\displaystyle f}\n\n being a linear function to be learned:\n\n\n\n\nf\n(\n\nx\n\nj\n\n\n)\n=\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n=\nw\n⋅\n\nx\n\nj\n\n\n\n\n{\\displaystyle f(x_{j})=\\langle w,x_{j}\\rangle =w\\cdot x_{j}}\n\n\nwhere \n\n\n\n\nx\n\nj\n\n\n∈\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle x_{j}\\in \\mathbb {R} ^{d}}\n\n is a vector of inputs (data points) and \n\n\n\nw\n∈\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle w\\in \\mathbb {R} ^{d}}\n\n is a linear filter vector.\nThe goal is to compute the filter vector \n\n\n\nw\n\n\n{\\displaystyle w}\n\n.\nTo this end, a square loss function \n\n\n\n\nV\n(\nf\n(\n\nx\n\nj\n\n\n)\n,\n\ny\n\nj\n\n\n)\n=\n(\nf\n(\n\nx\n\nj\n\n\n)\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n=\n(\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\\langle w,x_{j}\\rangle -y_{j})^{2}}\n\n\nis used to compute the vector \n\n\n\nw\n\n\n{\\displaystyle w}\n\n that minimizes the empirical loss\n\n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n=\n\n∑\n\nj\n=\n1\n\n\nn\n\n\nV\n(\n⟨\nw\n,\n\nx\n\nj\n\n\n⟩\n,\n\ny\n\nj\n\n\n)\n=\n\n∑\n\nj\n=\n1\n\n\nn\n\n\n(\n\nx\n\nj\n\n\n\nT\n\n\n\nw\n−\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle I_{n}[w]=\\sum _{j=1}^{n}V(\\langle w,x_{j}\\rangle ,y_{j})=\\sum _{j=1}^{n}(x_{j}^{\\mathsf {T}}w-y_{j})^{2}}\n\n \nwhere\n\n\n\n\n\ny\n\nj\n\n\n∈\n\nR\n\n.\n\n\n{\\displaystyle y_{j}\\in \\mathbb {R} .}\n\n\n Let \n\n\n\nX\n\n\n{\\displaystyle X}\n\n be the \n\n\n\ni\n×\nd\n\n\n{\\displaystyle i\\times d}\n\n data matrix and \n\n\n\ny\n∈\n\n\nR\n\n\ni\n\n\n\n\n{\\displaystyle y\\in \\mathbb {R} ^{i}}\n\n is the column vector of target values after the arrival of the first \n\n\n\ni\n\n\n{\\displaystyle i}\n\n data points.\nAssuming that the covariance matrix \n\n\n\n\nΣ\n\ni\n\n\n=\n\nX\n\n\nT\n\n\n\nX\n\n\n{\\displaystyle \\Sigma _{i}=X^{\\mathsf {T}}X}\n\n is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution \n\n\n\n\nf\n\n∗\n\n\n(\nx\n)\n=\n⟨\n\nw\n\n∗\n\n\n,\nx\n⟩\n\n\n{\\displaystyle f^{*}(x)=\\langle w^{*},x\\rangle }\n\n to the linear least squares problem is given by\n\n\n\n\n\nw\n\n∗\n\n\n=\n(\n\nX\n\n\nT\n\n\n\nX\n\n)\n\n−\n1\n\n\n\nX\n\n\nT\n\n\n\ny\n=\n\nΣ\n\ni\n\n\n−\n1\n\n\n\n∑\n\nj\n=\n1\n\n\ni\n\n\n\nx\n\nj\n\n\n\ny\n\nj\n\n\n.\n\n\n{\\displaystyle w^{*}=(X^{\\mathsf {T}}X)^{-1}X^{\\mathsf {T}}y=\\Sigma _{i}^{-1}\\sum _{j=1}^{i}x_{j}y_{j}.}\n\n\n Now, calculating the covariance matrix \n\n\n\n\nΣ\n\ni\n\n\n=\n\n∑\n\nj\n=\n1\n\n\ni\n\n\n\nx\n\nj\n\n\n\nx\n\nj\n\n\n\nT\n\n\n\n\n\n{\\displaystyle \\Sigma _{i}=\\sum _{j=1}^{i}x_{j}x_{j}^{\\mathsf {T}}}\n\n takes time \n\n\n\nO\n(\ni\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(id^{2})}\n\n, inverting the \n\n\n\nd\n×\nd\n\n\n{\\displaystyle d\\times d}\n\n matrix takes time \n\n\n\nO\n(\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(d^{3})}\n\n, while the rest of the multiplication takes time \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n, giving a total time of \n\n\n\nO\n(\ni\n\nd\n\n2\n\n\n+\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(id^{2}+d^{3})}\n\n. When there are \n\n\n\nn\n\n\n{\\displaystyle n}\n\n total points in the dataset, to recompute the solution after the arrival of every datapoint \n\n\n\ni\n=\n1\n,\n…\n,\nn\n\n\n{\\displaystyle i=1,\\ldots ,n}\n\n, the naive approach will have a total complexity \n\n\n\nO\n(\n\nn\n\n2\n\n\n\nd\n\n2\n\n\n+\nn\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(n^{2}d^{2}+nd^{3})}\n\n. Note that when storing the matrix \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n, then updating it at each step needs only adding \n\n\n\n\nx\n\ni\n+\n1\n\n\n\nx\n\ni\n+\n1\n\n\n\nT\n\n\n\n\n\n{\\displaystyle x_{i+1}x_{i+1}^{\\mathsf {T}}}\n\n, which takes \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n time, reducing the total time to \n\n\n\nO\n(\nn\n\nd\n\n2\n\n\n+\nn\n\nd\n\n3\n\n\n)\n=\nO\n(\nn\n\nd\n\n3\n\n\n)\n\n\n{\\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}\n\n, but with an additional storage space of \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n to store \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n.[1]\n The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising \n\n\n\n\n\nw\n\n0\n\n\n=\n0\n∈\n\n\nR\n\n\nd\n\n\n\n\n\n{\\displaystyle \\textstyle w_{0}=0\\in \\mathbb {R} ^{d}}\n\n and \n\n\n\n\n\nΓ\n\n0\n\n\n=\nI\n∈\n\n\nR\n\n\nd\n×\nd\n\n\n\n\n\n{\\displaystyle \\textstyle \\Gamma _{0}=I\\in \\mathbb {R} ^{d\\times d}}\n\n, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:\n\n\n\n\n\nΓ\n\ni\n\n\n=\n\nΓ\n\ni\n−\n1\n\n\n−\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\nx\n\ni\n\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\n\n1\n+\n\nx\n\ni\n\n\n\nT\n\n\n\n\nΓ\n\ni\n−\n1\n\n\n\nx\n\ni\n\n\n\n\n\n\n\n{\\displaystyle \\Gamma _{i}=\\Gamma _{i-1}-{\\frac {\\Gamma _{i-1}x_{i}x_{i}^{\\mathsf {T}}\\Gamma _{i-1}}{1+x_{i}^{\\mathsf {T}}\\Gamma _{i-1}x_{i}}}}\n\n\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nΓ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)}\n\n\nThe above iteration algorithm can be proved using induction on \n\n\n\ni\n\n\n{\\displaystyle i}\n\n.[2] The proof also shows that \n\n\n\n\nΓ\n\ni\n\n\n=\n\nΣ\n\ni\n\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{i}=\\Sigma _{i}^{-1}}\n\n. One can look at RLS also in the context of adaptive filters (see RLS).\n The complexity for \n\n\n\nn\n\n\n{\\displaystyle n}\n\n steps of this algorithm is \n\n\n\nO\n(\nn\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(nd^{2})}\n\n, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step \n\n\n\ni\n\n\n{\\displaystyle i}\n\n here are to store the matrix \n\n\n\n\nΓ\n\ni\n\n\n\n\n{\\displaystyle \\Gamma _{i}}\n\n, which is constant at \n\n\n\nO\n(\n\nd\n\n2\n\n\n)\n\n\n{\\displaystyle O(d^{2})}\n\n. For the case when \n\n\n\n\nΣ\n\ni\n\n\n\n\n{\\displaystyle \\Sigma _{i}}\n\n is not invertible, consider the regularised version of the problem loss function \n\n\n\n\n∑\n\nj\n=\n1\n\n\nn\n\n\n\n\n(\n\n\nx\n\nj\n\n\n\nT\n\n\n\nw\n−\n\ny\n\nj\n\n\n\n)\n\n\n2\n\n\n+\nλ\n\n\n‖\nw\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle \\sum _{j=1}^{n}\\left(x_{j}^{\\mathsf {T}}w-y_{j}\\right)^{2}+\\lambda \\left\\|w\\right\\|_{2}^{2}}\n\n. Then, it's easy to show that the same algorithm works with \n\n\n\n\nΓ\n\n0\n\n\n=\n(\nI\n+\nλ\nI\n\n)\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{0}=(I+\\lambda I)^{-1}}\n\n, and the iterations proceed to give \n\n\n\n\nΓ\n\ni\n\n\n=\n(\n\nΣ\n\ni\n\n\n+\nλ\nI\n\n)\n\n−\n1\n\n\n\n\n{\\displaystyle \\Gamma _{i}=(\\Sigma _{i}+\\lambda I)^{-1}}\n\n.[1]\n When this \n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nΓ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\Gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)}\n\n \nis replaced by\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n\nx\n\ni\n\n\n\n(\n\n\nx\n\ni\n\n\n\nT\n\n\n\n\nw\n\ni\n−\n1\n\n\n−\n\ny\n\ni\n\n\n\n)\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n∇\nV\n(\n⟨\n\nw\n\ni\n−\n1\n\n\n,\n\nx\n\ni\n\n\n⟩\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\gamma _{i}x_{i}\\left(x_{i}^{\\mathsf {T}}w_{i-1}-y_{i}\\right)=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{i}\\rangle ,y_{i})}\n\n \nor \n\n\n\n\nΓ\n\ni\n\n\n∈\n\n\nR\n\n\nd\n×\nd\n\n\n\n\n{\\displaystyle \\Gamma _{i}\\in \\mathbb {R} ^{d\\times d}}\n\n by \n\n\n\n\nγ\n\ni\n\n\n∈\n\nR\n\n\n\n{\\displaystyle \\gamma _{i}\\in \\mathbb {R} }\n\n, this becomes the stochastic gradient descent algorithm. In this case, the complexity for \n\n\n\nn\n\n\n{\\displaystyle n}\n\n steps of this algorithm reduces to \n\n\n\nO\n(\nn\nd\n)\n\n\n{\\displaystyle O(nd)}\n\n. The storage requirements at every step \n\n\n\ni\n\n\n{\\displaystyle i}\n\n are constant at \n\n\n\nO\n(\nd\n)\n\n\n{\\displaystyle O(d)}\n\n.\n However, the stepsize \n\n\n\n\nγ\n\ni\n\n\n\n\n{\\displaystyle \\gamma _{i}}\n\n needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size \n\n\n\n\nγ\n\ni\n\n\n≈\n\n\n1\n\ni\n\n\n\n,\n\n\n{\\displaystyle \\gamma _{i}\\approx {\\frac {1}{\\sqrt {i}}},}\n\n one can prove the convergence of the average iterate \n\n\n\n\n\n\nw\n¯\n\n\n\nn\n\n\n=\n\n\n1\nn\n\n\n\n∑\n\ni\n=\n1\n\n\nn\n\n\n\nw\n\ni\n\n\n\n\n{\\textstyle {\\overline {w}}_{n}={\\frac {1}{n}}\\sum _{i=1}^{n}w_{i}}\n\n. This setting is a special case of stochastic optimization, a well known problem in optimization.[1]\n In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is called incremental gradient method and corresponds to an iteration\n\n\n\n\n\nw\n\ni\n\n\n=\n\nw\n\ni\n−\n1\n\n\n−\n\nγ\n\ni\n\n\n∇\nV\n(\n⟨\n\nw\n\ni\n−\n1\n\n\n,\n\nx\n\n\nt\n\ni\n\n\n\n\n⟩\n,\n\ny\n\n\nt\n\ni\n\n\n\n\n)\n\n\n{\\displaystyle w_{i}=w_{i-1}-\\gamma _{i}\\nabla V(\\langle w_{i-1},x_{t_{i}}\\rangle ,y_{t_{i}})}\n\n \nThe main difference with the stochastic gradient method is that here a sequence \n\n\n\n\nt\n\ni\n\n\n\n\n{\\displaystyle t_{i}}\n\n is chosen to decide which training point is visited in the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk.[3] Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.[1]\n Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method. This discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction [1] that if \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n is the data matrix and \n\n\n\n\nw\n\ni\n\n\n\n\n{\\displaystyle w_{i}}\n\n is the output after \n\n\n\ni\n\n\n{\\displaystyle i}\n\n steps of the SGD algorithm, then,\n\n\n\n\n\nw\n\ni\n\n\n=\n\nX\n\ni\n\n\n\nT\n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle w_{i}=X_{i}^{\\mathsf {T}}c_{i}}\n\n \nwhere \n\n\n\n\nc\n\ni\n\n\n=\n(\n(\n\nc\n\ni\n\n\n\n)\n\n1\n\n\n,\n(\n\nc\n\ni\n\n\n\n)\n\n2\n\n\n,\n.\n.\n.\n,\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n)\n∈\n\n\nR\n\n\ni\n\n\n\n\n{\\displaystyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\\in \\mathbb {R} ^{i}}\n\n and the sequence \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n satisfies the recursion:\n\n\n\n\n\nc\n\n0\n\n\n=\n0\n\n\n{\\displaystyle c_{0}=0}\n\n\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\nj\n\n\n=\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n,\nj\n=\n1\n,\n2\n,\n.\n.\n.\n,\ni\n−\n1\n\n\n{\\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}\n\n and\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n=\n\nγ\n\ni\n\n\n\n\n(\n\n\n\ny\n\ni\n\n\n−\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n⟨\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n⟩\n\n\n)\n\n\n\n\n{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x_{i}\\rangle {\\Big )}}\n\n\nNotice that here \n\n\n\n⟨\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n⟩\n\n\n{\\displaystyle \\langle x_{j},x_{i}\\rangle }\n\n is just the standard Kernel on \n\n\n\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{d}}\n\n, and the predictor is of the form \n\n\n\n\n\nf\n\ni\n\n\n(\nx\n)\n=\n⟨\n\nw\n\ni\n−\n1\n\n\n,\nx\n⟩\n=\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\n⟨\n\nx\n\nj\n\n\n,\nx\n⟩\n.\n\n\n{\\displaystyle f_{i}(x)=\\langle w_{i-1},x\\rangle =\\sum _{j=1}^{i-1}(c_{i-1})_{j}\\langle x_{j},x\\rangle .}\n\n\n \nNow, if a general kernel \n\n\n\nK\n\n\n{\\displaystyle K}\n\n is introduced instead and let the predictor be \n\n\n\n\n\nf\n\ni\n\n\n(\nx\n)\n=\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\nK\n(\n\nx\n\nj\n\n\n,\nx\n)\n\n\n{\\displaystyle f_{i}(x)=\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}\n\n\nthen the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to\n\n\n\n\n(\n\nc\n\ni\n\n\n\n)\n\ni\n\n\n=\n\nγ\n\ni\n\n\n\n\n(\n\n\n\ny\n\ni\n\n\n−\n\n∑\n\nj\n=\n1\n\n\ni\n−\n1\n\n\n(\n\nc\n\ni\n−\n1\n\n\n\n)\n\nj\n\n\nK\n(\n\nx\n\nj\n\n\n,\n\nx\n\ni\n\n\n)\n\n\n)\n\n\n\n\n{\\displaystyle (c_{i})_{i}=\\gamma _{i}{\\Big (}y_{i}-\\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\\Big )}}\n\n\nThe above expression requires storing all the data for updating \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n. The total time complexity for the recursion when evaluating for the \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-th datapoint is \n\n\n\nO\n(\n\nn\n\n2\n\n\nd\nk\n)\n\n\n{\\displaystyle O(n^{2}dk)}\n\n, where \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is the cost of evaluating the kernel on a single pair of points.[1] Thus, the use of the kernel has allowed the movement from a finite dimensional parameter space \n\n\n\n\n\nw\n\ni\n\n\n∈\n\n\nR\n\n\nd\n\n\n\n\n\n{\\displaystyle \\textstyle w_{i}\\in \\mathbb {R} ^{d}}\n\n to a possibly infinite dimensional feature represented by a kernel \n\n\n\nK\n\n\n{\\displaystyle K}\n\n by instead performing the recursion on the space of parameters \n\n\n\n\n\nc\n\ni\n\n\n∈\n\n\nR\n\n\ni\n\n\n\n\n\n{\\displaystyle \\textstyle c_{i}\\in \\mathbb {R} ^{i}}\n\n, whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.[1]\n Online convex optimization (OCO) [4] is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:\n For \n\n\n\nt\n=\n1\n,\n2\n,\n.\n.\n.\n,\nT\n\n\n{\\displaystyle t=1,2,...,T}\n\n\n The goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point \n\n\n\nu\n∈\nS\n\n\n{\\displaystyle u\\in S}\n\n in hindsight. As an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set \n\n\n\nS\n=\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle S=\\mathbb {R} ^{d}}\n\n, and nature sends back the convex loss function \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n(\n⟨\nw\n,\n\nx\n\nt\n\n\n⟩\n−\n\ny\n\nt\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle v_{t}(w)=(\\langle w,x_{t}\\rangle -y_{t})^{2}}\n\n. Note here that \n\n\n\n\ny\n\nt\n\n\n\n\n{\\displaystyle y_{t}}\n\n is implicitly sent with \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n.\n Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.[citation needed]\n Some simple online convex optimisation algorithms are:\n The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and round \n\n\n\nt\n\n\n{\\displaystyle t}\n\n is simply given by:\n\n\n\n\n\nw\n\nt\n\n\n=\n\n\n\na\nr\ng\n\nm\ni\nn\n\n\n\nw\n∈\nS\n\n\n⁡\n\n∑\n\ni\n=\n1\n\n\nt\n−\n1\n\n\n\nv\n\ni\n\n\n(\nw\n)\n\n\n{\\displaystyle w_{t}=\\mathop {\\operatorname {arg\\,min} } _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)}\n\n\nThis method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n\n\n‖\n\nw\n−\n\nx\n\nt\n\n\n\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle v_{t}(w)=\\left\\|w-x_{t}\\right\\|_{2}^{2}}\n\n), one can show a regret bound that grows as \n\n\n\nlog\n⁡\n(\nT\n)\n\n\n{\\displaystyle \\log(T)}\n\n. However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.\n This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function \n\n\n\nR\n:\nS\n→\n\nR\n\n\n\n{\\displaystyle R:S\\to \\mathbb {R} }\n\n is chosen and learning performed in round t as follows:\n\n\n\n\n\nw\n\nt\n\n\n=\n\n\n\na\nr\ng\n\nm\ni\nn\n\n\n\nw\n∈\nS\n\n\n⁡\n\n∑\n\ni\n=\n1\n\n\nt\n−\n1\n\n\n\nv\n\ni\n\n\n(\nw\n)\n+\nR\n(\nw\n)\n\n\n{\\displaystyle w_{t}=\\mathop {\\operatorname {arg\\,min} } _{w\\in S}\\sum _{i=1}^{t-1}v_{i}(w)+R(w)}\n\n\nAs a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n⟨\nw\n,\n\nz\n\nt\n\n\n⟩\n\n\n{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n\n. Also, let \n\n\n\nS\n=\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle S=\\mathbb {R} ^{d}}\n\n. Suppose the regularisation function \n\n\n\nR\n(\nw\n)\n=\n\n\n1\n\n2\nη\n\n\n\n\n\n‖\nw\n‖\n\n\n2\n\n\n2\n\n\n\n\n{\\textstyle R(w)={\\frac {1}{2\\eta }}\\left\\|w\\right\\|_{2}^{2}}\n\n is chosen for some positive number \n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n. Then, one can show that the regret minimising iteration becomes \n\n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n−\nη\n\n∑\n\ni\n=\n1\n\n\nt\n\n\n\nz\n\ni\n\n\n=\n\nw\n\nt\n\n\n−\nη\n\nz\n\nt\n\n\n\n\n{\\displaystyle w_{t+1}=-\\eta \\sum _{i=1}^{t}z_{i}=w_{t}-\\eta z_{t}}\n\n\nNote that this can be rewritten as \n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nw\n\nt\n\n\n−\nη\n∇\n\nv\n\nt\n\n\n(\n\nw\n\nt\n\n\n)\n\n\n{\\displaystyle w_{t+1}=w_{t}-\\eta \\nabla v_{t}(w_{t})}\n\n, which looks exactly like online gradient descent.\n If S is instead some convex subspace of \n\n\n\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{d}}\n\n, S would need to be projected onto, leading to the modified update rule\n\n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nΠ\n\nS\n\n\n(\n−\nη\n\n∑\n\ni\n=\n1\n\n\nt\n\n\n\nz\n\ni\n\n\n)\n=\n\nΠ\n\nS\n\n\n(\nη\n\nθ\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle w_{t+1}=\\Pi _{S}(-\\eta \\sum _{i=1}^{t}z_{i})=\\Pi _{S}(\\eta \\theta _{t+1})}\n\n\nThis algorithm is known as lazy projection, as the vector \n\n\n\n\nθ\n\nt\n+\n1\n\n\n\n\n{\\displaystyle \\theta _{t+1}}\n\n accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n, and thus the average regret goes to 0 as desired.\n The above proved a regret bound for linear loss functions \n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\n⟨\nw\n,\n\nz\n\nt\n\n\n⟩\n\n\n{\\displaystyle v_{t}(w)=\\langle w,z_{t}\\rangle }\n\n. To generalise the algorithm to any convex loss function, the subgradient \n\n\n\n∂\n\nv\n\nt\n\n\n(\n\nw\n\nt\n\n\n)\n\n\n{\\displaystyle \\partial v_{t}(w_{t})}\n\n of \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n is used as a linear approximation to \n\n\n\n\nv\n\nt\n\n\n\n\n{\\displaystyle v_{t}}\n\n near \n\n\n\n\nw\n\nt\n\n\n\n\n{\\displaystyle w_{t}}\n\n, leading to the online subgradient descent algorithm:\n Initialise parameter \n\n\n\nη\n,\n\nw\n\n1\n\n\n=\n0\n\n\n{\\displaystyle \\eta ,w_{1}=0}\n\n\n For \n\n\n\nt\n=\n1\n,\n2\n,\n.\n.\n.\n,\nT\n\n\n{\\displaystyle t=1,2,...,T}\n\n\n One can use the OSD algorithm to derive \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n regret bounds for the online version of SVM's for classification, which use the hinge loss\n\n\n\n\nv\n\nt\n\n\n(\nw\n)\n=\nmax\n{\n0\n,\n1\n−\n\ny\n\nt\n\n\n(\nw\n⋅\n\nx\n\nt\n\n\n)\n}\n\n\n{\\displaystyle v_{t}(w)=\\max\\{0,1-y_{t}(w\\cdot x_{t})\\}}\n\n\n Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent. The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm. For the Euclidean regularisation, one can show a regret bound of \n\n\n\nO\n(\n\n\nT\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {T}})}\n\n, which can be improved further to a \n\n\n\nO\n(\nlog\n⁡\nT\n)\n\n\n{\\displaystyle O(\\log T)}\n\n for strongly convex and exp-concave loss functions.\n Continual learning means constantly improving the learned model by processing continuous streams of information.[5] Continual learning capabilities are essential for software systems and autonomous agents interacting in an ever changing real world. However, continual learning is a challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting. \n The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions \n\n\n\n\nf\n\n1\n\n\n,\n\nf\n\n2\n\n\n,\n…\n,\n\nf\n\nn\n\n\n\n\n{\\displaystyle f_{1},f_{2},\\ldots ,f_{n}}\n\n. The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by\n\n\n\n\n\nw\n\nt\n\n\n=\n\nw\n\nt\n−\n1\n\n\n−\n\nγ\n\nt\n\n\n∇\nV\n(\n⟨\n\nw\n\nt\n−\n1\n\n\n,\n\nx\n\nt\n\n\n⟩\n,\n\ny\n\nt\n\n\n)\n\n\n{\\displaystyle w_{t}=w_{t-1}-\\gamma _{t}\\nabla V(\\langle w_{t-1},x_{t}\\rangle ,y_{t})}\n\n\n The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n defined above.[6] Indeed, in the case of an infinite stream of data, since the examples \n\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n(\n\nx\n\n2\n\n\n,\n\ny\n\n2\n\n\n)\n,\n…\n\n\n{\\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\\ldots }\n\n are assumed to be drawn i.i.d. from the distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n, the sequence of gradients of \n\n\n\nV\n(\n⋅\n,\n⋅\n)\n\n\n{\\displaystyle V(\\cdot ,\\cdot )}\n\n in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation \n\n\n\nI\n[\n\nw\n\nt\n\n\n]\n−\nI\n[\n\nw\n\n∗\n\n\n]\n\n\n{\\displaystyle I[w_{t}]-I[w^{\\ast }]}\n\n, where \n\n\n\n\nw\n\n∗\n\n\n\n\n{\\displaystyle w^{\\ast }}\n\n is the minimizer of \n\n\n\nI\n[\nw\n]\n\n\n{\\displaystyle I[w]}\n\n.[7] This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.\n The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method.[3] In this case, one instead looks at the empirical risk:\n\n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n=\n\n\n1\nn\n\n\n\n∑\n\ni\n=\n1\n\n\nn\n\n\nV\n(\n⟨\nw\n,\n\nx\n\ni\n\n\n⟩\n,\n\ny\n\ni\n\n\n)\n \n.\n\n\n{\\displaystyle I_{n}[w]={\\frac {1}{n}}\\sum _{i=1}^{n}V(\\langle w,x_{i}\\rangle ,y_{i})\\ .}\n\n\nSince the gradients of \n\n\n\nV\n(\n⋅\n,\n⋅\n)\n\n\n{\\displaystyle V(\\cdot ,\\cdot )}\n\n in the incremental gradient descent iterations are also stochastic estimates of the gradient of \n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n\n\n{\\displaystyle I_{n}[w]}\n\n, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations \n\n\n\n\nI\n\nn\n\n\n[\n\nw\n\nt\n\n\n]\n−\n\nI\n\nn\n\n\n[\n\nw\n\nn\n\n\n∗\n\n\n]\n\n\n{\\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\\ast }]}\n\n, where \n\n\n\n\nw\n\nn\n\n\n∗\n\n\n\n\n{\\displaystyle w_{n}^{\\ast }}\n\n is the minimizer of \n\n\n\n\nI\n\nn\n\n\n[\nw\n]\n\n\n{\\displaystyle I_{n}[w]}\n\n.\n Learning paradigms\n General algorithms\n Learning models\n",
        "url": "https://en.wikipedia.org/wiki/Batch_learning"
    },
    {
        "id": 14,
        "title": "Curriculum learning",
        "content": "Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of \"difficulty\" may be provided externally or discovered automatically as part of the training process. This is intended to attain good performance more quickly, or to converge to a better local optimum if the global optimum is not found.[1][2]\n Most generally, curriculum learning  is the technique of successively increasing the difficulty of examples in the training set that is presented to a model over multiple training iterations. This can produce better results than exposing the model to the full training set immediately under some circumstances; most typically, when the model is able to learn general principles from easier examples, and then gradually incorporate more complex and nuanced information as harder examples are introduced, such as edge cases. This has been shown to work in many domains, most likely as a form of regularization.[3]\n There are several major variations in how the technique is applied:\n Since curriculum learning only concerns the selection and ordering of training data, it can be combined with many other techniques in machine learning. The success of the method assumes that a model trained for an easier version of the problem can generalize to harder versions, so it can be seen as a form of transfer learning. Some authors also consider curriculum learning to include other forms of progressively increasing complexity, such as increasing the number of model parameters.[11] It is frequently combined with reinforcement learning, such as learning a simplified version of a game first.[12]\n Some domains have shown success with anti-curriculum learning: training on the most difficult examples first. One example is the ACCAN method for speech recognition, which trains on the examples with the lowest signal-to-noise ratio first.[13]\n The term \"curriculum learning\" was introduced by Yoshua Bengio et al in 2009,[14] with reference to the psychological technique of shaping in animals and structured education for humans: beginning with the simplest concepts and then building on them. The authors also note that the application of this technique in machine learning has its roots in the early study of neural networks such as Jeffrey Elman's 1993 paper Learning and development in neural networks: the importance of starting small. [15] Bengio et al showed good results for problems in image classification, such as identifying geometric shapes with progressively more complex forms, and language modeling, such as training with a gradually expanding vocabulary. They conclude that, for curriculum strategies, \"their beneficial effect is most pronounced on the test\nset\", suggesting good generalization.\n The technique has since been applied to many other domains:\n",
        "url": "https://en.wikipedia.org/wiki/Curriculum_learning"
    },
    {
        "id": 15,
        "title": "Rule-based machine learning",
        "content": "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.\n Rule-based machine learning approaches include learning classifier systems,[4] association rule learning,[5] artificial immune systems,[6] and any other method that relies on a set of rules, each covering contextual knowledge.\n While rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers. This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set.\n Rules typically take the form of an '{IF:THEN} expression', (e.g. {IF 'condition' THEN 'result'}, or as a more specific example, {IF 'red' AND 'octagon' THEN 'stop-sign}). An individual rule is not in itself a model, since the rule is only applicable when its condition is satisfied. Therefore rule-based machine learning methods typically comprise a set of rules, or knowledge base, that collectively make up the prediction model.\n",
        "url": "https://en.wikipedia.org/wiki/Rule-based_machine_learning"
    },
    {
        "id": 16,
        "title": "Neuro-symbolic AI",
        "content": "Neuro-symbolic AI is a type of artificial intelligence that integrates neural and symbolic AI architectures to address the weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Leslie Valiant[1] and others,[2][3] the effective construction of rich computational cognitive models demands the combination of symbolic reasoning and efficient machine learning. Gary Marcus argued, \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\"[4] Further, \"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much useful knowledge is abstract to proceed without tools that represent and manipulate abstraction, and to date, the only known machinery that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"[5]\n Henry Kautz,[6] Francesca Rossi,[7] and Bart Selman[8] also argued for a synthesis. Their arguments attempt to address the two kinds of thinking, as discussed in Daniel Kahneman's book Thinking Fast and Slow. It describes cognition as encompassing two components: System 1 is fast, reflexive, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition while symbolic reasoning best handles the second kind. Both are needed for a robust, reliable AI that can learn, reason, and interact with humans to accept advice and answer questions. Such dual-process models with explicit references to the two contrasting systems have been worked on since the 1990s, both in AI and in Cognitive Science, by multiple researchers.[9]\n Approaches for integration are diverse.[10] Henry Kautz's taxonomy of neuro-symbolic architectures[11] follows, along with some examples:\n These categories are not exhaustive, as they do not consider multi-agent systems. In 2005, Bader and Hitzler presented a more fine-grained categorization that considered, e.g., whether the use of symbols included logic and if it did, whether the logic was propositional or first-order logic.[15] The 2005 categorization and Kautz's taxonomy above are compared and contrasted in a 2021 article.[11] Recently, Sepp Hochreiter argued that Graph Neural Networks \"...are the predominant models of neural-symbolic computing\"[16] since \"[t]hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle-particle interactions.\"[17]\n Gary Marcus argues that \"...hybrid architectures that combine learning and symbol manipulation are necessary for robust intelligence, but not sufficient\",[18] and that there are\n ...four cognitive prerequisites for building robust artificial intelligence: \n This echoes earlier calls for hybrid models as early as the 1990s.[20][21]\n Garcez and Lamb described research in this area as ongoing at least since the 1990s.[22][23] At that time, the terms symbolic and sub-symbolic AI were popular.\n A series of workshops on neuro-symbolic AI has been held annually since 2005 Neuro-Symbolic Artificial Intelligence.[24] In the early 1990s, an initial set of workshops on this topic were organized.[20]\n Key research questions remain,[25] such as:\n Implementations of neuro-symbolic approaches include:\n",
        "url": "https://en.wikipedia.org/wiki/Neuro-symbolic_AI"
    },
    {
        "id": 17,
        "title": "Neuromorphic computing",
        "content": "\n Neuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.[1][2] A neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.[3][4] In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). Recent advances have even discovered ways to mimic the human nervous system through liquid solutions of chemical systems.[5]\n A key aspect of neuromorphic engineering is understanding how the morphology of individual neurons, circuits, applications, and overall architectures creates desirable computations, affects how information is represented, influences robustness to damage, incorporates learning and development, adapts to local change (plasticity), and facilitates evolutionary change.\n Neuromorphic engineering is an interdisciplinary subject that takes inspiration from biology, physics, mathematics, computer science, and electronic engineering[4] to design artificial neural systems, such as vision systems, head-eye systems, auditory processors, and autonomous robots, whose physical architecture and design principles are based on those of biological nervous systems.[6] One of the first applications for neuromorphic engineering was proposed by Carver Mead[7] in the late 1980s.\n Neuromorphic engineering is for now set apart by the inspiration it takes from what we know about the structure and operations of the brain. Neuromorphic engineering translates what we know about the brain's function into computer systems. Work has mostly focused on replicating the analog nature of biological computation and the role of neurons in cognition.\n The biological processes of neurons and their synapses are dauntingly complex, and thus very difficult to artificially simulate. A key feature of biological brains is that all of the processing in neurons uses analog chemical signals. This makes it hard to replicate brains in computers because the current generation of computers is completely digital. However, the characteristics of these  chemical signals can be abstracted into mathematical functions that closely capture the essence of the neuron's operations.\n The goal of neuromorphic computing is not to perfectly mimic the brain and all of its functions, but instead to extract what is known of its structure and operations to be used in a practical computing system. No neuromorphic system will claim nor attempt to reproduce every element of neurons and synapses, but all adhere to the idea that computation is highly distributed throughout a series of small computing elements analogous to a neuron. While this sentiment is standard, researchers chase this goal with different methods.[8]\n The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors,[9] spintronic memories, threshold switches, transistors,[10][4] among others. The implementation details overlap with the concepts of Reservoir Computation. Training software-based neuromorphic systems of spiking neural networks can be achieved using error backpropagation, e.g. using Python-based frameworks such as snnTorch,[11] or using canonical learning rules from the biological learning literature, e.g. using BindsNet.[12]\n As early as 2006, researchers at Georgia Tech published a field programmable neural array.[13] This chip was the first in a line of increasingly complex arrays of floating gate transistors that allowed programmability of charge on the gates of MOSFETs to model the channel-ion characteristics of neurons in the brain and was one of the first cases of a silicon programmable array of neurons.\n In November 2011, a group of MIT researchers created a computer chip that mimics the analog, ion-based communication in a synapse between two neurons using 400 transistors and standard CMOS manufacturing techniques.[14][15]\n In June 2012, spintronic researchers at Purdue University presented a paper on the design of a neuromorphic chip using lateral spin valves and memristors. They argue that the architecture works similarly to neurons and can therefore be used to test methods of reproducing the brain's processing. In addition, these chips are significantly more energy-efficient than conventional ones.[16]\n Research at HP Labs on Mott memristors has shown that while they can be non-volatile, the volatile behavior exhibited at temperatures significantly below the phase transition temperature can be exploited to fabricate a neuristor,[17] a biologically-inspired device that mimics behavior found in neurons.[17] In September 2013, they presented models and simulations that show how the spiking behavior of these neuristors can be used to form the components required for a Turing machine.[18]\n Neurogrid, built by Brains in Silicon at Stanford University,[19] is an example of hardware designed using neuromorphic engineering principles. The circuit board is composed of 16 custom-designed chips, referred to as NeuroCores. Each NeuroCore's analog circuitry is designed to emulate neural elements for 65536 neurons, maximizing energy efficiency. The emulated neurons are connected using digital circuitry designed to maximize spiking throughput.[20][21]\n A research project with implications for neuromorphic engineering is the Human Brain Project that is attempting to simulate a complete human brain in a supercomputer using biological data. It is made up of a group of researchers in neuroscience, medicine, and computing.[22] Henry Markram, the project's co-director, has stated that the project proposes to establish a foundation to explore and understand the brain and its diseases, and to use that knowledge to build new computing technologies. The three primary goals of the project are to better understand how the pieces of the brain fit and work together, to understand how to objectively diagnose and treat brain diseases and to use the understanding of the human brain to develop neuromorphic computers. Since the simulation of a complete human brain will require a powerful supercomputer,  the current focus on neuromorphic computers is being encouraged.[23] $1.3 billion has been allocated to the project by The European Commission.[24]\n Other research with implications for neuromorphic engineering involve the BRAIN Initiative[25] and the TrueNorth chip from IBM.[26] Neuromorphic devices have also been demonstrated using nanocrystals, nanowires, and conducting polymers.[27] There also is development of a memristive device for quantum neuromorphic architectures.[28] In 2022, researchers at MIT have reported the development of brain-inspired artificial synapses, using the ion proton (H+), for 'analog deep learning'.[29][30]\n Intel unveiled its neuromorphic research chip, called \"Loihi\", in October 2017. The chip uses an asynchronous spiking neural network (SNN) to implement adaptive self-modifying event-driven fine-grained parallel computations used to implement learning and inference with high efficiency.[31][32]\n IMEC, a Belgium-based nanoelectronics research center, demonstrated the world's first self-learning neuromorphic chip. The brain-inspired chip, based on OxRAM technology, has the capability of self-learning and has been demonstrated to have the ability to compose music.[33] IMEC released the 30-second tune composed by the prototype. The chip was sequentially loaded with songs in the same time signature and style. The songs were old Belgian and French flute minuets, from which the chip learned the rules at play and then applied them.[34]\n The Blue Brain Project, led by Henry Markram, aims to build biologically detailed digital reconstructions and simulations of the mouse brain. The Blue Brain Project has created in silico models of rodent brains, while attempting to replicate as many details about its biology as possible. The supercomputer-based simulations offer new perspectives on understanding the structure and functions of the brain.\n The European Union funded a series of projects at the University of Heidelberg, which led to the development of BrainScaleS (brain-inspired multiscale computation in neuromorphic hybrid systems), a hybrid analog neuromorphic supercomputer located at Heidelberg University, Germany. It was developed as part of the Human Brain Project neuromorphic computing platform and is the complement to the SpiNNaker supercomputer (which is based on digital technology). The architecture used in BrainScaleS mimics biological neurons and their connections on a physical level; additionally, since the components are made of silicon, these model neurons operate on average 864 times (24 hours of real time is 100 seconds in the machine simulation) faster than that of their biological counterparts.[35]\n In 2019, the European Union funded the project \"Neuromorphic quantum computing\"[36] exploring the use of neuromorphic computing to perform quantum operations. Neuromorphic quantum computing[37] (abbreviated as 'n.quantum computing') is an unconventional computing type of computing that uses neuromorphic computing to perform quantum operations.[38][39] It was suggested that quantum algorithms, which are algorithms that run on a realistic model of quantum computation, can be computed equally efficiently with neuromorphic quantum computing.[40][41][42][43][44] Both, traditional quantum computing and neuromorphic quantum computing are physics-based unconventional computing approaches to computations and do not follow the von Neumann architecture. They both construct a system (a circuit) that represents the physical problem at hand, and then leverage their respective physics properties of the system to seek the \"minimum\". Neuromorphic quantum computing and quantum computing share similar physical properties during computation.[44][45]\n Brainchip announced in October 2021 that it was taking orders for its Akida AI Processor Development Kits[46] and in January 2022 that it was taking orders for its Akida AI Processor PCIe boards,[47] making it the world's first commercially available neuromorphic processor.\n Neuromemristive systems are a subclass of neuromorphic computing systems that focuses on the use of memristors to implement neuroplasticity. While neuromorphic engineering focuses on mimicking biological behavior, neuromemristive systems focus on abstraction.[48] For example, a neuromemristive system may replace the details of a cortical microcircuit's behavior with an abstract neural network model.[49]\n There exist several neuron inspired threshold logic functions[9] implemented with memristors that have applications in high level pattern recognition applications. Some of the applications reported recently include speech recognition,[50] face recognition[51] and object recognition.[52] They also find applications in replacing conventional digital logic gates.[53][54]\n For (quasi)ideal passive memristive circuits, the evolution of the memristive memories can be written in a closed form (Caravelli–Traversa–Di Ventra equation):[55][56]\n as a function of the properties of the physical memristive network and the external sources. The equation is valid for the case of the Williams-Strukov original toy model, as  in the case of ideal memristors, \n\n\n\nα\n=\n0\n\n\n{\\displaystyle \\alpha =0}\n\n. However, the hypothesis of the existence of an ideal memristor is debatable.[57] In the equation above, \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n is the \"forgetting\" time scale constant, typically associated to memory volatility, while \n\n\n\nχ\n=\n\n\n\n\nR\n\noff\n\n\n−\n\nR\n\non\n\n\n\n\nR\n\noff\n\n\n\n\n\n\n{\\displaystyle \\chi ={\\frac {R_{\\text{off}}-R_{\\text{on}}}{R_{\\text{off}}}}}\n\n is the ratio of off and on values of the limit resistances of the memristors, \n\n\n\n\n\n\nS\n→\n\n\n\n\n\n{\\displaystyle {\\vec {S}}}\n\n is the vector of the sources of the circuit and \n\n\n\nΩ\n\n\n{\\displaystyle \\Omega }\n\n is a projector on the fundamental loops of the circuit. The constant \n\n\n\nβ\n\n\n{\\displaystyle \\beta }\n\n has the dimension of a voltage and is associated to the properties of the memristor; its physical origin is the charge mobility in the conductor. The diagonal matrix and vector \n\n\n\nX\n=\ndiag\n⁡\n(\n\n\n\nX\n→\n\n\n\n)\n\n\n{\\displaystyle X=\\operatorname {diag} ({\\vec {X}})}\n\n and \n\n\n\n\n\n\nX\n→\n\n\n\n\n\n{\\displaystyle {\\vec {X}}}\n\n respectively, are instead the internal value of the memristors, with values between 0 and 1. This equation thus requires adding extra constraints on the memory values in order to be reliable.\n It has been recently shown that the equation above exhibits tunneling phenomena and used to study Lyapunov functions.[58][56]\n The concept of neuromorphic systems can be extended to sensors (not just to computation). An example of this applied to detecting light is the retinomorphic sensor or, when employed in an array, the event camera. An event camera's pixels all register changes in brightness levels individually, which makes these cameras comparable to human eyesight in their theoretical power consumption.[59] In 2022, researchers from the Max Planck Institute for Polymer Research reported an organic artificial spiking neuron that exhibits the signal diversity of biological neurons while operating in the biological wetware, thus enabling in-situ neuromorphic sensing and biointerfacing applications.[60][61]\n The Joint Artificial Intelligence Center, a branch of the U.S. military, is a center dedicated to the procurement and implementation of AI software and neuromorphic hardware for combat use. Specific applications include smart headsets/goggles and robots. JAIC intends to rely heavily on neuromorphic technology to connect \"every sensor (to) every shooter\" within a network of neuromorphic-enabled units.\n While the interdisciplinary concept of neuromorphic engineering is relatively new, many of the same ethical considerations apply to neuromorphic systems as apply to human-like machines and artificial intelligence in general. However, the fact that neuromorphic systems are designed to mimic a human brain gives rise to unique ethical questions surrounding their usage.\n However, the practical debate is that neuromorphic hardware as well as artificial \"neural networks\" are immensely simplified models of how the brain operates or processes information at a much lower complexity in terms of size and functional technology and a much more regular structure in terms of connectivity. Comparing neuromorphic chips to the brain is a very crude comparison similar to comparing a plane to a bird just because they both have wings and a tail. The fact is that biological neural cognitive systems are many orders of magnitude more energy- and compute-efficient than current state-of-the-art AI and neuromorphic engineering is an attempt to narrow this gap by inspiring from the brain's mechanism just like many engineering designs have bio-inspired features.\n Significant ethical limitations may be placed on neuromorphic engineering due to public perception.[62] Special Eurobarometer 382: Public Attitudes Towards Robots, a survey conducted by the European Commission, found that 60% of European Union citizens wanted a ban of robots in the care of children, the elderly, or the disabled. Furthermore, 34% were in favor of a ban on robots in education, 27% in healthcare, and 20% in leisure. The European Commission classifies these areas as notably “human.” The report cites increased public concern with robots that are able to mimic or replicate human functions. Neuromorphic engineering, by definition, is designed to replicate the function of the human brain.[63]\n The social concerns surrounding neuromorphic engineering are likely to become even more profound in the future. The European Commission found that EU citizens between the ages of 15 and 24 are more likely to think of robots as human-like (as opposed to instrument-like) than EU citizens over the age of 55. When presented an image of a robot that had been defined as human-like, 75% of EU citizens aged 15–24 said it corresponded with the idea they had of robots while only 57% of EU citizens over the age of 55 responded the same way. The human-like nature of neuromorphic systems, therefore, could place them in the categories of robots many EU citizens would like to see banned in the future.[63]\n As neuromorphic systems have become increasingly advanced, some scholars[who?] have advocated for granting personhood rights to these systems.  Daniel Lim, a critic of technology development in the Human Brain Project, which aims to advance brain-inspired computing, has argued that advancement in neuromorphic computing could lead to machine consciousness or personhood.[64]  If these systems are to be treated as people, then many tasks humans perform using neuromorphic systems, including their termination, may be morally impermissible as these acts would violate their autonomy.[64]\n There is significant legal debate around property rights and artificial intelligence. In Acohs Pty Ltd v. Ucorp Pty Ltd, Justice Christopher Jessup of the Federal Court of Australia found that the source code for Material Safety Data Sheets could not be copyrighted as it was generated by a software interface rather than a human author.[65] The same question may apply to neuromorphic systems: if a neuromorphic system successfully mimics a human brain and produces a piece of original work, who, if anyone, should be able to claim ownership of the work?[66]\n",
        "url": "https://en.wikipedia.org/wiki/Neuromorphic_engineering"
    },
    {
        "id": 18,
        "title": "Quantum machine learning",
        "content": "Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]\n The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning.[9][10][11] While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program.[12] This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device.[13][14][15] These routines can be more complex in nature and executed faster on a quantum computer.[7] Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.[16][17]\n Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system[18][19] or creating new quantum experiments.[20][21][22]\n Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.[23][24][25]\n Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".[26][27]\n Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\n Associative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\n Typical classical associative memories store p patterns in the \n\n\n\nO\n(\n\nn\n\n2\n\n\n)\n\n\n{\\displaystyle O(n^{2})}\n\n interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\n Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \n\n\n\np\n≤\nO\n(\nn\n)\n\n\n{\\displaystyle p\\leq O(n)}\n\n.\n Quantum associative memories[2][3][4] (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is \n\n\n\nO\n(\np\nn\n)\n\n\n{\\displaystyle O(pn)}\n\n. One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.\n A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations.[30][31][32] Since a state of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n qubits is described by \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input.\n Many quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations[33] (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse[34] or low rank.[35] For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. \n\n\n\nO\n\n\n\n(\n\nn\n\n2.373\n\n\n)\n\n\n\n\n\n{\\displaystyle O{\\mathord {\\left(n^{2.373}\\right)}}}\n\n), but they are not restricted to sparse matrices.\n Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression,[31][32] the least-squares version of support vector machines,[30] and Gaussian processes.[36]\n A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases,[37][38] this step easily hides the complexity of the task.[39][40]\n VQAs are one of the most studied classes of quantum algorithms, as modern research demonstrates their applicability to the vast majority of known major applications of the quantum computer, and they appear to be a leading hope for gaining quantum supremacy.[41]  VQAs are a mixed quantum-classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer. VQAs are considered best for NISQ as VQAs are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits. Researchers have studied circuit-based algorithms to solve optimization problems and find the ground state energy of complex systems, which were difficult to solve or required a large time to perform the computation using a classical computer.[42][43]\n Variational Quantum Circuits also known as Parametrized Quantum Circuits (PQCs) are based on Variational Quantum Algorithms (VQAs). VQCs consist of three parts: preparation of initial states, quantum circuit, and measurement. Researchers are extensively studying VQCs, as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts. It is theoretically and numerically proven that we can approximate non-linear functions, like those used in neural networks, on quantum circuits. Due to VQCs superiority, neural network has been replaced by VQCs in Reinforcement Learning tasks and Generative Algorithms. The intrinsic nature of quantum devices towards decoherence, random gate error and measurement errors caused to have high potential to limit the training of the variation circuits. Training the VQCs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training.[44][45][46]\n Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In quantum machine learning, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space.[47][48] By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.[49]\n Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians[50] and the k-nearest neighbors algorithms.[9] Other applications include quadratic speedups in the training of perceptron[51] and the computation of attention.[52]\n An example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least \n\n\n\n\n\nO\n\n\n\n(\n\n\n\nn\nk\n\n\n\n)\n\n\n\n{\\displaystyle {\\mathcal {O}}\\left({\\sqrt {\\frac {n}{k}}}\\right)}\n\n compared to classical versions of k-medians, where \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is the number of data points and \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is the number of clusters.[50]\n Amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm[53] as well as the performance of reinforcement learning agents in the projective simulation framework.[54]\n Reinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements.[55][54][56] In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent,[54] or due to the possibility to probe the environment in superpositions,[29] a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions[57] and superconducting circuits.[58] A quantum speedup of the agent's internal decision-making time[54] has been experimentally demonstrated in trapped ions,[59] while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.[60]\n Quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.\n As the depth of the quantum circuit advances on NISQ devices, the noise level rises, posing a significant challenge to accurately computing costs and gradients on training models. The noise tolerance will be improved by using the quantum perceptron and the quantum algorithm on the currently accessible quantum hardware.[citation needed]\n A regular connection of similar components known as neurons forms the basis of even the most complex brain networks. Typically, a neuron has two operations: the inner product and an activation function. As opposed to the activation function, which is typically nonlinear, the inner product is a linear process. With quantum computing, linear processes may be easily accomplished additionally,  due to the simplicity of implementation, the threshold function is preferred by the majority of quantum neurons for activation functions.[citation needed]\n Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.\n A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.[61]\n Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.[62][63][64] The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.\n The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.[63] Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.[62] The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets.[65] In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.[66]\n Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.[67] Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.[65][64][68]\n Quantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing.[69] The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template.[70][19] This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.\n Quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutsch's model of a quantum computational network.[71] Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set.[71] Such gates make certain phases unable to be observed and generate specific oscillations.[71] Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing.[72] Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size.[72] A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time.[71] The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10 - 20 nm)[73] electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of \n\n\n\n\n|\n\n0\n⟩\n\n\n{\\displaystyle |0\\rangle }\n\n  and \n\n\n\n\n|\n\n1\n⟩\n\n\n{\\displaystyle |1\\rangle }\n\n in Dirac notation.[73]\n A novel design for multi-dimensional vectors that uses circuits as convolution filters[74] is QCNN. It was inspired by the advantages of CNNs[75][76] and the power of QML. It is made using a combination of a variational quantum circuit(VQC)[77] and a deep neural network[78](DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ[79] devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.[80]\n The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts  that make up the quantum convolutional filter are:  the encoder, the parameterized quantum circuit (PQC),[81] and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.\n Quantum neural networks take advantage of the hierarchical structures,[82] and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid \"barren plateau,\" one of the most significant issues with PQC-based algorithms, ensuring trainability.[83] Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.  Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.[84]\n Dissipative QNNs (DQNNs) are constructed from layers of qubits coupled by perceptron called building blocks, which have an arbitrary unitary design. Each node in the network layer of a DQNN is given a distinct collection of qubits, and each qubit is also given a unique quantum perceptron unitary to characterize it.[85][86] The input states information are transported through the network in a feed-forward fashion, layer-to-layer transition mapping on the qubits of the two adjacent layers, as the name implies. Dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer.[87] When performing a broad supervised learning task, DQNN are used to learn a unitary matrix connecting the input and output quantum states. The training data for this task consists of the quantum state and the corresponding classical labels.\n Inspired by the extremely successful classical Generative adversarial network(GAN),[88] dissipative quantum generative adversarial network (DQGAN) is introduced for unsupervised learning of the unlabeled training data . The generator and the discriminator are the two DQNNs that make up a single DQGAN.[86] The generator's goal is to create false training states that the discriminator cannot differentiate from the genuine ones, while the discriminator's objective is to separate the real training states from the fake states created by the generator. The relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set. DQGAN has a fully quantum architecture and is trained in quantum data.\n Hidden quantum Markov models[89] (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well.[90] Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: density matrices. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived.[90] Additionally, since classical HMMs are a particular kind of Bayes net, an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous Bayesian inference, which should allow for the general construction of the quantum versions of probabilistic graphical models.[90]\n In the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.\n One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case.[91] (This also relates to work on quantum pattern matching.[92]) The problem of learning unitary transformations can be approached in a similar way.[93]\n Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum.[94] Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in,[29] where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.[60]\n The need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning (or XQML[95] in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML).[96] XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage.[97] For example, XQML has been used in the context of mobile malware detection and classification.[98] Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach.[95] For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations)[99] has also been proposed, known as Q-LIME.[100]\n The term \"quantum machine learning\" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians[101] and automatically generating quantum experiments.[20]\n Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.[102]\n The starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as \n\n\n\n{\n0\n,\n1\n\n}\n\nn\n\n\n\n\n{\\displaystyle \\{0,1\\}^{n}}\n\n. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.\n In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more.[103] If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).[103]\n A natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples \n\n\n\n\n∑\n\nx\n\n\n\n\nD\n(\nx\n)\n\n\n\n|\n\nx\n,\nc\n(\nx\n)\n⟩\n\n\n{\\displaystyle \\sum _{x}{\\sqrt {D(x)}}|x,c(x)\\rangle }\n\n. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors.[104] However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution.[105] When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).[103]\n This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.[106]\n The earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009.[107] Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer.[108][109] A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.[65]\n Using a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation.[110] NMR technology also enables universal quantum computing,[citation needed] and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state  quantum computer in 2015.[111] The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.\n Photonic implementations are attracting more attention,[112] not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013.[113] Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule.[114] A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.[115]\n Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor.[116] This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed,[117] and an experiment with quantum dots performed.[118] A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.\n Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.\n In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs).[119] However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found.[120] The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.\n A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.[59]\n In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment.[121][60] The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.\n While machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, quantum machine learning remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient.[citation needed] Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random.[122] This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction.\n Many of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen[123] collected some of the statements made by well known scientists in the field:\n",
        "url": "https://en.wikipedia.org/wiki/Quantum_machine_learning"
    },
    {
        "id": 19,
        "title": "Statistical classification",
        "content": "When classification is performed by a computer, statistical methods are normally used to develop the algorithm.\n Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\n An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\n Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis.\n Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value.  Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.\n A common subclass of classification is probabilistic classification.  Algorithms of this nature use statistical inference to find the best class for a given instance.  Unlike other algorithms, which simply output a \"best\" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers:\n Early work on statistical classification was undertaken by Fisher,[1][2] in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation.[3] This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule should be linear.[3][4] Later work for the multivariate normal distribution allowed the classifier to be nonlinear:[5] several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.\n Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population.[6] Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.[7]\n Some Bayesian procedures involve the calculation of  group-membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation.\n Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.[8] Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.\n Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance.  Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent).  Features may variously be binary (e.g. \"on\" or \"off\"); categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type); ordinal (e.g. \"large\", \"medium\" or \"small\"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10).\n A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product.  The predicted category is the one with the highest score.  This type of score function is known as a linear predictor function and has the following general form:\n\n\n\n\nscore\n⁡\n(\n\n\nX\n\n\ni\n\n\n,\nk\n)\n=\n\n\nβ\n\n\nk\n\n\n⋅\n\n\nX\n\n\ni\n\n\n,\n\n\n{\\displaystyle \\operatorname {score} (\\mathbf {X} _{i},k)={\\boldsymbol {\\beta }}_{k}\\cdot \\mathbf {X} _{i},}\n\n\nwhere Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k.  In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k.\n Algorithms with this basic setup are known as linear classifiers.  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.\n Examples of such algorithms include\n Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms has been developed. The most commonly used include:[9]\n Choices between different possible algorithms are frequently made on the basis of quantitative evaluation of accuracy.\n Classification has many applications. In some of these, it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.\n",
        "url": "https://en.wikipedia.org/wiki/Statistical_classification"
    },
    {
        "id": 20,
        "title": "Generative model",
        "content": "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):\n The distinction between these last two classes is not consistently made;[4] Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes.[5] Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\n Standard examples of each, all of which are linear classifiers, are:\n In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n\n\n\nP\n(\nY\n\n|\n\nX\n=\nx\n)\n\n\n{\\displaystyle P(Y|X=x)}\n\n (discriminative model), and base classification on that; or one can estimate the joint distribution \n\n\n\nP\n(\nX\n,\nY\n)\n\n\n{\\displaystyle P(X,Y)}\n\n (generative model), from that compute the conditional probability \n\n\n\nP\n(\nY\n\n|\n\nX\n=\nx\n)\n\n\n{\\displaystyle P(Y|X=x)}\n\n, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.\n An alternative division defines these symmetrically as:\n Regardless of precise definition, the terminology is constitutional because a generative model can be used to \"generate\" random instances (outcomes), either of an observation and target \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n, or of an observation x given a target value y,[2] while a discriminative model or discriminative classifier (without a model) can be used to \"discriminate\" the value of the target variable Y, given an observation x.[3] The difference between \"discriminate\" (distinguish) and \"classify\" is subtle, and these are not consistently distinguished. (The term \"discriminative classifier\" becomes a pleonasm when \"discrimination\" is equivalent to \"classification\".)\n The term \"generative model\" is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables. Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.\n In application to classification, the observable X is frequently a continuous variable, the target Y is generally a discrete variable consisting of a finite set of labels, and the conditional probability \n\n\n\nP\n(\nY\n∣\nX\n)\n\n\n{\\displaystyle P(Y\\mid X)}\n\n can also be interpreted as a (non-deterministic) target function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f\\colon X\\to Y}\n\n, considering X as inputs and Y as outputs.\n Given a finite set of labels, the two definitions of \"generative model\" are closely related. A model of the conditional distribution \n\n\n\nP\n(\nX\n∣\nY\n=\ny\n)\n\n\n{\\displaystyle P(X\\mid Y=y)}\n\n is a model of the distribution of each label, and a model of the joint distribution is equivalent to a model of the distribution of label values \n\n\n\nP\n(\nY\n)\n\n\n{\\displaystyle P(Y)}\n\n, together with the distribution of observations given a label, \n\n\n\nP\n(\nX\n∣\nY\n)\n\n\n{\\displaystyle P(X\\mid Y)}\n\n; symbolically, \n\n\n\nP\n(\nX\n,\nY\n)\n=\nP\n(\nX\n∣\nY\n)\nP\n(\nY\n)\n.\n\n\n{\\displaystyle P(X,Y)=P(X\\mid Y)P(Y).}\n\n Thus, while a model of the joint probability distribution is more informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.\n Given a model of the joint distribution, \n\n\n\nP\n(\nX\n,\nY\n)\n\n\n{\\displaystyle P(X,Y)}\n\n, the distribution of the individual variables can be computed as the marginal distributions \n\n\n\nP\n(\nX\n)\n=\n\n∑\n\ny\n\n\nP\n(\nX\n,\nY\n=\ny\n)\n\n\n{\\displaystyle P(X)=\\sum _{y}P(X,Y=y)}\n\n and \n\n\n\nP\n(\nY\n)\n=\n\n∫\n\nx\n\n\nP\n(\nY\n,\nX\n=\nx\n)\n\n\n{\\displaystyle P(Y)=\\int _{x}P(Y,X=x)}\n\n (considering X as continuous, hence integrating over it, and Y as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: \n\n\n\nP\n(\nX\n∣\nY\n)\n=\nP\n(\nX\n,\nY\n)\n\n/\n\nP\n(\nY\n)\n\n\n{\\displaystyle P(X\\mid Y)=P(X,Y)/P(Y)}\n\n and \n\n\n\nP\n(\nY\n∣\nX\n)\n=\nP\n(\nX\n,\nY\n)\n\n/\n\nP\n(\nX\n)\n\n\n{\\displaystyle P(Y\\mid X)=P(X,Y)/P(X)}\n\n.\n Given a model of one conditional probability, and estimated probability distributions for the variables X and Y, denoted \n\n\n\nP\n(\nX\n)\n\n\n{\\displaystyle P(X)}\n\n and \n\n\n\nP\n(\nY\n)\n\n\n{\\displaystyle P(Y)}\n\n, one can estimate the opposite conditional probability using Bayes' rule:\n For example, given a generative model for \n\n\n\nP\n(\nX\n∣\nY\n)\n\n\n{\\displaystyle P(X\\mid Y)}\n\n, one can estimate:\n and given a discriminative model for \n\n\n\nP\n(\nY\n∣\nX\n)\n\n\n{\\displaystyle P(Y\\mid X)}\n\n, one can estimate:\n Note that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well.\n A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn \n\n\n\np\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle p(y|x)}\n\n directly from the data and then try to classify data. On the other hand, generative algorithms try to learn \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n which can be transformed into \n\n\n\np\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle p(y|x)}\n\n later to classify the data. One of the advantages of generative algorithms is that you can use \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n to generate new data similar to existing data. On the other hand, it has been proved that some discriminative algorithms give better performance than some generative algorithms in classification tasks.[6]\n Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. But in general, they don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.[7]\n With the rise of deep learning, a new family of methods, called deep generative models (DGMs),[8][9] is formed through the combination of generative models and deep neural networks. An increase in the scale of the neural networks is typically accompanied by an increase in the scale of the training data, both of which are required for good performance.[10]\n Popular DGMs include variational autoencoders (VAEs), generative adversarial networks (GANs), and auto-regressive models. Recently, there has been a trend to build very large deep generative models.[8] For example, GPT-3, and its precursor GPT-2,[11] are auto-regressive neural language models that contain billions of parameters, BigGAN[12] and VQ-VAE[13] which are used for image generation that can have hundreds of millions of parameters, and Jukebox is a very large generative model for musical audio that contains billions of parameters.[14]\n Types of generative models are:\n If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case.\n Suppose the input data is \n\n\n\nx\n∈\n{\n1\n,\n2\n}\n\n\n{\\displaystyle x\\in \\{1,2\\}}\n\n, the set of labels for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is \n\n\n\ny\n∈\n{\n0\n,\n1\n}\n\n\n{\\displaystyle y\\in \\{0,1\\}}\n\n, and there are the following 4 data points:\n\n\n\n\n(\nx\n,\ny\n)\n=\n{\n(\n1\n,\n0\n)\n,\n(\n1\n,\n1\n)\n,\n(\n2\n,\n0\n)\n,\n(\n2\n,\n1\n)\n}\n\n\n{\\displaystyle (x,y)=\\{(1,0),(1,1),(2,0),(2,1)\\}}\n\n\n For the above data, estimating the joint probability distribution \n\n\n\np\n(\nx\n,\ny\n)\n\n\n{\\displaystyle p(x,y)}\n\n from the empirical measure will be the following:\n while \n\n\n\np\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle p(y|x)}\n\n will be following:\n Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with \"representing and speedily is an good\"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.\n",
        "url": "https://en.wikipedia.org/wiki/Generative_model"
    }
]