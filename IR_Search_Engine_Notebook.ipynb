{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac0826e",
   "metadata": {},
   "source": [
    "# Μηχανή Αναζήτησης με βάση την Wikepedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb2eda",
   "metadata": {},
   "source": [
    "Αυτό το Notebook περιγράφει τη διαδικασία σχεδιασμού, υλοποίησης και αξιολόγησης μιας απλής μηχανής αναζήτησης με βάση την Ανάκτηση Πληροφορίας. Θα δούμε βήμα-βήμα πώς επεξεργαστήκαμε τα δεδομένα, υλοποιήσαμε αλγόριθμους αναζήτησης και αξιολογήσαμε την απόδοσή τους."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bba01",
   "metadata": {},
   "source": [
    "## Προετοιμασία"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b05ac",
   "metadata": {},
   "source": [
    "Πριν ξεκινήσουμε να εκτελούμε τα ζητούμενα της άσησης, πρέπει να κάνουμε τα κατάληλλα installs, οπως είναι το nltk και το BeautifulSoup, το οποιο θα χρησημοποιηθεί στο πρώτο βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff78305",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8ed00",
   "metadata": {},
   "source": [
    "## Βημα 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e13a",
   "metadata": {},
   "source": [
    "Στην συνέχεια πραγματαποιούμαι εναν web-crawler με την χρήση της βιβλιοθήκης Beautiful Soup. Ετσι βρίσκουμε, με την βοήθεια keywords, και αποθηκεύουμε 20 σχετικά άρθρα για περεταίρω ανάλυση.\n",
    "Παρακάτω φαίνεται η εκτέλεση αυτών:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a400dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching articles for keyword: machine learning\n",
      "Found article #1: Main Page\n",
      "Found article #2: Machine learning\n",
      "Found article #3: Machine Learning (journal)\n",
      "Found article #4: Statistical learning in language acquisition\n",
      "Found article #5: Data mining\n",
      "Found article #6: Supervised learning\n",
      "Found article #7: Unsupervised learning\n",
      "Found article #8: Weak supervision\n",
      "Found article #9: Self-supervised learning\n",
      "Found article #10: Reinforcement learning\n",
      "Found article #11: Meta-learning (computer science)\n",
      "Found article #12: Online machine learning\n",
      "Found article #13: Online machine learning\n",
      "Found article #14: Curriculum learning\n",
      "Found article #15: Rule-based machine learning\n",
      "Found article #16: Neuro-symbolic AI\n",
      "Found article #17: Neuromorphic computing\n",
      "Found article #18: Quantum machine learning\n",
      "Found article #19: Statistical classification\n",
      "Found article #20: Generative model\n",
      "Searching articles for keyword: data science\n",
      "Searching articles for keyword: artificial intelligence\n",
      "Searching articles for keyword: neural networks\n",
      "Searching articles for keyword: deep learning\n",
      "Συλλογή ολοκληρώθηκε και αποθηκεύτηκε σε αρχείο JSON.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def get_wikipedia_articles(base_url, keywords, total_articles=20):\n",
    "    articles = []\n",
    "    visited_urls = set()  # Χρησιμοποιείται για την αποφυγή διπλότυπων\n",
    "    article_count = 0\n",
    "\n",
    "    for keyword in keywords:\n",
    "        #if article_count >= total_articles:\n",
    "          #  break\n",
    "\n",
    "        search_url = f\"{base_url}/w/index.php?search={keyword}\"\n",
    "        print(f\"Searching articles for keyword: {keyword}\")\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Εύρεση συνδέσμων άρθρων\n",
    "        links = soup.select(\"a[href^='/wiki/']\")\n",
    "        for link in links:\n",
    "            if article_count >= total_articles:\n",
    "                break\n",
    "\n",
    "            href = link.get('href')\n",
    "            if not href.startswith('/wiki/') or ':' in href:  # Αγνοήστε ειδικές σελίδες\n",
    "                continue\n",
    "\n",
    "            article_url = base_url + href\n",
    "            if article_url in visited_urls:  # Αν έχει ήδη επισκεφθεί\n",
    "                continue\n",
    "\n",
    "            visited_urls.add(article_url)  # Προσθέστε στο σύνολο\n",
    "            article_response = requests.get(article_url)\n",
    "            article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "\n",
    "            # Εξαγωγή τίτλου και περιεχομένου\n",
    "            title = article_soup.find(\"h1\").text\n",
    "            paragraphs = article_soup.find_all(\"p\")\n",
    "            content = \" \".join([p.text for p in paragraphs])\n",
    "\n",
    "            print(f\"Found article #{article_count + 1}: {title}\")\n",
    "            articles.append({\n",
    "                \"id\": article_count + 1,  # Αριθμός άρθρου\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"url\": article_url\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Παράδειγμα χρήσης\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://en.wikipedia.org\"\n",
    "    keywords = [\"machine learning\", \"data science\", \"artificial intelligence\", \"neural networks\", \"deep learning\"]\n",
    "    data = get_wikipedia_articles(base_url, keywords, total_articles=20)\n",
    "\n",
    "    # Αποθήκευση σε JSON\n",
    "    with open(\"wikipedia_articles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Συλλογή ολοκληρώθηκε και αποθηκεύτηκε σε αρχείο JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72118656",
   "metadata": {},
   "source": [
    "## Βήμα 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dde06e",
   "metadata": {},
   "source": [
    "Για την προεπεξεργασία εφαρμόζουμε: tokenization, αφαίρεση stop-words, και stemming/lemmatization. Αυτά τα βήματα βοηθούν στη μείωση του μεγέθους του λεξιλογίου και στη βελτίωση της απόδοσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fb6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα δεδομένα προεπεξεργάστηκαν και αποθηκεύτηκαν στο processed_wikipedia_articles.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Κατεβάστε δεδομένα για NLTK (αν χρειάζεται)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Αφαίρεση ειδικών χαρακτήρων\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # 2. Μετατροπή σε πεζά\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Αφαίρεση stop-words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_dataset(input_file, output_file):\n",
    "    # Φόρτωση δεδομένων από το JSON\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Καθαρισμός περιεχομένου\n",
    "    for article in data:\n",
    "        article['processed_content'] = preprocess_text(article['content'])\n",
    "\n",
    "    # Αποθήκευση καθαρισμένων δεδομένων σε νέο αρχείο\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Τα δεδομένα προεπεξεργάστηκαν και αποθηκεύτηκαν στο {output_file}\")\n",
    "\n",
    "# Παράδειγμα χρήσης\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_dataset(\"wikipedia_articles.json\", \"processed_wikipedia_articles.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7b99f",
   "metadata": {},
   "source": [
    "Ακολουθούν οι πρώτες 20 γραμμες του .json αρχείου για να δειχθεί η μορφή και να παραμείνει ευανάγνωστο το αρχείο. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Main Page\",\n",
    "        \"content\": \"January 19\\n The roadside hawk (Rupornis magnirostris) is a relatively small bird of prey in the family Accipitridae. It is found from Mexico through Central America and in most of South America east of the Andes. With the possible exception of dense rainforests, the roadside hawk is well adapted to most ecosystems in its range. It is also an urban bird, and is possibly the most common species of hawk seen in various cities throughout its range. This roadside hawk of the subspecies R. m. griseocauda was photographed feeding on a speckled racer in Orange Walk District, Belize.\\n Photograph credit: Charles J. Sharp Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:\\n This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.\\n\",\n",
    "        \"url\": \"https://en.wikipedia.org/wiki/Main_Page\",\n",
    "        \"processed_content\": [\n",
    "            \"january\",\n",
    "            \"roadside\",\n",
    "            \"hawk\",\n",
    "            \"rupornis\",\n",
    "            \"magnirostris\",\n",
    "            \"relatively\",\n",
    "            \"small\",\n",
    "            \"bird\",\n",
    "            \"prey\",\n",
    "            \"family\",\n",
    "            \"accipitridae\",\n",
    "            \"found\",\n",
    "            \"mexico\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86daf8",
   "metadata": {},
   "source": [
    "## Βήμα 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dd6a1",
   "metadata": {},
   "source": [
    "Δημιουργούμε ένα ανεστραμμένο ευρετήριο που αποθηκεύει τους όρους και τα εγγραφα στα οποία εμφανίζεται ο κάθε όρος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fdfacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(input_file, output_file):\n",
    "    # Φόρτωση επεξεργασμένων δεδομένων\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Ανεστραμμένο ευρετήριο\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for article in data:\n",
    "        doc_id = article[\"id\"]\n",
    "        processed_content = article[\"processed_content\"]\n",
    "\n",
    "        for word in processed_content:\n",
    "            if doc_id not in inverted_index[word]:\n",
    "                inverted_index[word].append(doc_id)\n",
    "\n",
    "    # Αποθήκευση του ανεστραμμένου ευρετηρίου\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο {output_file}\")\n",
    "\n",
    "# Παράδειγμα χρήσης\n",
    "if __name__ == \"__main__\":\n",
    "    create_inverted_index(\"processed_wikipedia_articles.json\", \"inverted_index.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307453b",
   "metadata": {},
   "source": [
    "Παραθέτονται παραδείγματα απο το inverted_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e911a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"listed\": [\n",
    "        1\n",
    "    ],\n",
    "    \"machine\": [\n",
    "        2,\n",
    "        3,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        11,\n",
    "        12,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19\n",
    "    ],\n",
    "    \"learning\": [\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8,\n",
    "        9,\n",
    "        10,\n",
    "        11,\n",
    "        12,\n",
    "        13,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "        18,\n",
    "        19,\n",
    "        20\n",
    "    ],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc85369",
   "metadata": {},
   "source": [
    "## Βήμα 4.α"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bf0cf",
   "metadata": {},
   "source": [
    "Υλοποιούμε μια μηχανή αναζήτησης για απλά ερωτήματα και boolean εντολές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Καλώς ήρθατε στη Μηχανή Αναζήτησης!\n",
      "Χρησιμοποιήστε λογικούς τελεστές (AND, OR, NOT) για πιο σύνθετα ερωτήματα.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "def load_data(index_file, articles_file):\n",
    "    with open(index_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        inverted_index = json.load(file)\n",
    "    with open(articles_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        articles = {article[\"id\"]: article for article in json.load(file)}\n",
    "    return inverted_index, articles\n",
    "\n",
    "def process_query(query):\n",
    "    tokens = query.lower().split()\n",
    "    processed_query = []\n",
    "    operators = {\"and\", \"or\", \"not\"}\n",
    "    for token in tokens:\n",
    "        if token in operators:\n",
    "            processed_query.append(token.upper())\n",
    "        else:\n",
    "            processed_query.append(token)\n",
    "    return processed_query\n",
    "\n",
    "def search(articles,inverted_index, query_tokens):\n",
    "    result_stack = []\n",
    "    all_doc_ids = set(articles.keys())  # Όλα τα έγγραφα στο ευρετήριο\n",
    "   \n",
    "    operators = []\n",
    "    operands = []\n",
    "\n",
    "    print(f\"Αναζήτηση για το ερώτημα: {' '.join(query_tokens)}\")\n",
    "\n",
    "    for token in query_tokens:\n",
    "   \n",
    "\n",
    "        if token == \"AND\":\n",
    "            operators.append(token)  \n",
    "        elif token == \"OR\":\n",
    "            operators.append(token)  \n",
    "        elif token == \"NOT\":\n",
    "            operators.append(token)  \n",
    "        else:\n",
    "            # Προσθέτουμε τον όρο στη στοίβα αν υπάρχει στο ευρετήριο\n",
    "            if token in inverted_index:\n",
    "                operands.append(set(inverted_index[token]))  \n",
    "            else:\n",
    "                operands.append(set())  \n",
    "                print(f\"Ο όρος '{token}' δεν βρέθηκε στο ευρετήριο, προστέθηκε κενό σύνολο.\")\n",
    "\n",
    "       \n",
    "    # Επεξεργασία των τελεστών μετά την προσθήκη όλων των όρων\n",
    "    while operators:\n",
    "        operator = operators.pop(0)  \n",
    "        if operator == \"AND\":\n",
    "            operand2 = operands.pop(0)  \n",
    "            operand1 = operands.pop(0)\n",
    "            operands.insert(0, operand1 & operand2)  # Τομή\n",
    "        elif operator == \"OR\":\n",
    "            operand2 = operands.pop(0)\n",
    "            operand1 = operands.pop(0)\n",
    "            operands.insert(0, operand1 | operand2)  # Ένωση\n",
    "        elif operator == \"NOT\":\n",
    "            operand1 = operands.pop(0)\n",
    "            result = all_doc_ids - operand1\n",
    "            print(f\"Result after NOT: {result}\")\n",
    "            operands.insert(0, result) \n",
    "            print(f\"Εφαρμόστηκε NOT, το αποτέλεσμα είναι: {result}\")\n",
    "    \n",
    "    if len(operands) != 1:\n",
    "        print(\"Σφάλμα: Το ερώτημα δεν επεξεργάστηκε σωστά.\")\n",
    "        return []\n",
    "\n",
    " \n",
    "    return operands.pop()\n",
    "\n",
    "def display_results(results, articles):\n",
    "    if not results:\n",
    "        print(\"Δεν βρέθηκαν σχετικά έγγραφα.\")\n",
    "    else:\n",
    "        print(f\"Βρέθηκαν {len(results)} έγγραφα:\")\n",
    "        for doc_id in results:\n",
    "            print(f\"- ID: {doc_id}, Title: {articles[doc_id]['title']}\")\n",
    "\n",
    "def main():\n",
    "    index_file = \"inverted_index.json\"\n",
    "    articles_file = \"processed_wikipedia_articles.json\"\n",
    "\n",
    "    inverted_index, articles = load_data(index_file, articles_file)\n",
    "\n",
    "    print(\"Καλώς ήρθατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"Χρησιμοποιήστε λογικούς τελεστές (AND, OR, NOT) για πιο σύνθετα ερωτήματα.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Εισάγετε το ερώτημά σας (ή 'exit' για έξοδο): \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Ευχαριστούμε που χρησιμοποιήσατε τη μηχανή αναζήτησης!\")\n",
    "            break\n",
    "\n",
    "        query_tokens = process_query(query)\n",
    "\n",
    "        results = search(articles,inverted_index, query_tokens)\n",
    "        display_results(results, articles)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c056e",
   "metadata": {},
   "source": [
    "Θα υπάρχει screenshot της ολοκληρωμένης εκτέλεσης στο έγγραφο τεκμηρίωσης, καθώς χρειάζεται user input. Παρακάτων φαίνεται αντιγραμμένη η έξοδος."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98250aea",
   "metadata": {},
   "source": [
    "Καλώς ήρθατε στη Μηχανή Αναζήτησης!\n",
    "Χρησιμοποιήστε λογικούς τελεστές (AND, OR, NOT) για πιο σύνθετα ερωτήματα.\n",
    "Εισάγετε το ερώτημά σας (ή 'exit' για έξοδο): machine\n",
    "Αναζήτηση για το ερώτημα: machine\n",
    "Βρέθηκαν 17 έγγραφα:\n",
    "- ID: 2, Title: Machine learning\n",
    "- ID: 3, Title: Machine Learning (journal)\n",
    "- ID: 5, Title: Data mining\n",
    "- ID: 6, Title: Supervised learning\n",
    "- ID: 7, Title: Unsupervised learning\n",
    "- ID: 8, Title: Weak supervision\n",
    "- ID: 9, Title: Self-supervised learning\n",
    "- ID: 10, Title: Reinforcement learning\n",
    "- ID: 11, Title: Meta-learning (computer science)\n",
    "- ID: 12, Title: Online machine learning\n",
    "- ID: 13, Title: Online machine learning\n",
    "- ID: 14, Title: Curriculum learning\n",
    "- ID: 15, Title: Rule-based machine learning\n",
    "- ID: 16, Title: Neuro-symbolic AI\n",
    "- ID: 17, Title: Neuromorphic computing\n",
    "- ID: 18, Title: Quantum machine learning\n",
    "- ID: 19, Title: Statistical classification\n",
    "Εισάγετε το ερώτημά σας (ή 'exit' για έξοδο): exit\n",
    "Ευχαριστούμε που χρησιμοποιήσατε τη μηχανή αναζήτησης!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab19c6d",
   "metadata": {},
   "source": [
    "## Βήμα 4.β"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e8e09",
   "metadata": {},
   "source": [
    "Ταξινομούμε τις εμφανίσεις της δοθείσας λέξης με βάση τους αντίστοιχους αλγορίθμους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d59eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Καλώς ήρθατε στη Μηχανή Αναζήτησης!\n",
      "Επιλέξτε τον αλγόριθμο ανάκτησης:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. BM25\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np  # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "from rank_bm25 import BM25Okapi  # type: ignore\n",
    "\n",
    "# Load the data (articles)\n",
    "def load_data(articles_file):\n",
    "    with open(articles_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        articles = {article[\"id\"]: article for article in json.load(file)}\n",
    "    return articles\n",
    "\n",
    "# Boolean Retrieval\n",
    "def search_boolean(query, documents):\n",
    "    query_terms = set(query.lower().split())\n",
    "    doc_scores = {}\n",
    "\n",
    "    for doc_idx, content in enumerate(documents):\n",
    "        terms_in_doc = set(content.lower().split())\n",
    "        if query_terms.issubset(terms_in_doc):  # All query terms must be in the document\n",
    "            doc_scores[doc_idx] = sum(content.lower().split().count(term) for term in query_terms)\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [doc[0] for doc in ranked_docs], [doc[1] for doc in ranked_docs]\n",
    "\n",
    "# TF-IDF Ranking\n",
    "\n",
    "\n",
    "def rank_with_tfidf(query, documents):\n",
    "    \"\"\"\n",
    "    Rank documents using TF-IDF and cosine similarity to the query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's search query.\n",
    "        documents (list of str): The list of document contents.\n",
    "\n",
    "    Returns:\n",
    "        ranked_docs (list of int): Indices of documents sorted by relevance.\n",
    "        scores (list of float): Corresponding cosine similarity scores.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer and fit on the documents\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Transform the query into the same TF-IDF space\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Compute cosine similarity between the query vector and all document vectors\n",
    "    cosine_similarities = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "\n",
    "    # Rank documents by descending similarity\n",
    "    ranked_docs = np.argsort(cosine_similarities)[::-1]\n",
    "    scores = cosine_similarities[ranked_docs]\n",
    "\n",
    "    return ranked_docs, scores\n",
    "\n",
    "\n",
    "\n",
    "# BM25 Ranking\n",
    "def rank_with_bm25(query, documents):\n",
    "    tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "    tokenized_query = query.lower().split()\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_docs = np.argsort(doc_scores)[::-1]\n",
    "    scores = [doc_scores[idx] for idx in ranked_docs]\n",
    "    return ranked_docs, scores\n",
    "\n",
    "def display_ranked_results(ranked_docs, scores, articles):\n",
    "    # Ensure results with non-zero scores are displayed\n",
    "    has_results = False\n",
    "    valid_results = [(doc_idx, score) for doc_idx, score in zip(ranked_docs, scores) if score != 0]\n",
    "\n",
    "\n",
    "    print(f\"Βρέθηκαν {len(valid_results)} σχετικά έγγραφα:\")\n",
    "    for i, (doc_idx, score) in enumerate(zip(ranked_docs, scores)):\n",
    "        if score != 0:  # Only display documents with a non-zero score\n",
    "            has_results = True\n",
    "            doc_id = list(articles.keys())[doc_idx]  # Map index to document ID\n",
    "            print(f\"- ID: {doc_id}, Score: {score:.4f}, Title: {articles[doc_id]['title']}\")\n",
    "\n",
    "    if not has_results:\n",
    "        print(\"Δεν βρέθηκαν σχετικά έγγραφα.\")\n",
    "\n",
    "\n",
    "def display_boolean_results(ranked_docs, scores, articles):\n",
    "    if not ranked_docs:  # Check if the list is empty\n",
    "        print(\"Δεν βρέθηκαν σχετικά έγγραφα.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Βρέθηκαν {len(ranked_docs)} έγγραφα:\")\n",
    "    for i, doc_idx in enumerate(ranked_docs):\n",
    "        doc_id = list(articles.keys())[doc_idx]\n",
    "        print(f\"- ID: {doc_id}, Occurrences: {scores[i]}, Title: {articles[doc_id]['title']}\")\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    articles_file = \"processed_wikipedia_articles.json\"\n",
    "    articles = load_data(articles_file)\n",
    "    documents = [article[\"content\"] for article in articles.values()]\n",
    "\n",
    "    print(\"Καλώς ήρθατε στη Μηχανή Αναζήτησης!\")\n",
    "    print(\"Επιλέξτε τον αλγόριθμο ανάκτησης:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. TF-IDF\")\n",
    "    print(\"3. BM25\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Εισάγετε το ερώτημά σας (ή 'exit' για έξοδο): \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Ευχαριστούμε που χρησιμοποιήσατε τη μηχανή αναζήτησης!\")\n",
    "            break\n",
    "\n",
    "        algo_choice = input(\"Επιλέξτε αλγόριθμο (1/2/3): \").strip()\n",
    "        if algo_choice == '1':\n",
    "            print(\"\\nΧρησιμοποιώντας Boolean Retrieval για την κατάταξη...\\n\")\n",
    "            ranked_docs, scores = search_boolean(query, documents)\n",
    "            display_boolean_results(ranked_docs, scores, articles)\n",
    "        elif algo_choice == '2':\n",
    "            print(\"\\nΧρησιμοποιώντας TF-IDF για την κατάταξη...\\n\")\n",
    "            ranked_docs, scores = rank_with_tfidf(query, documents)\n",
    "            display_ranked_results(ranked_docs, scores, articles)\n",
    "        elif algo_choice == '3':\n",
    "            print(\"\\nΧρησιμοποιώντας BM25 για την κατάταξη...\\n\")\n",
    "            ranked_docs, scores = rank_with_bm25(query, documents)\n",
    "            display_ranked_results(ranked_docs, scores, articles)\n",
    "        else:\n",
    "            print(\"Λανθασμένη επιλογή. Παρακαλώ δοκιμάστε ξανά.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482be4fd",
   "metadata": {},
   "source": [
    "Όπως και στο προηγούμενο υποερώτημα, θα υπάρχουν screenshot στο έγγραφο τεκμηρίωσης και εδώ φαίνεται αντιγραμμένη μία ενδεικτική είσοδος"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb64c4",
   "metadata": {},
   "source": [
    "Εισάγετε το ερώτημά σας (ή 'exit' για έξοδο): machine\n",
    "Επιλέξτε αλγόριθμο (1/2/3): 1\n",
    "\n",
    "Χρησιμοποιώντας Boolean Retrieval για την κατάταξη...\n",
    "\n",
    "Βρέθηκαν 17 έγγραφα:\n",
    "- ID: 2, Occurrences: 110, Title: Machine learning\n",
    "- ID: 18, Occurrences: 47, Title: Quantum machine learning\n",
    "- ID: 5, Occurrences: 8, Title: Data mining\n",
    "- ID: 15, Occurrences: 7, Title: Rule-based machine learning\n",
    "- ID: 12, Occurrences: 5, Title: Online machine learning\n",
    "- ID: 13, Occurrences: 5, Title: Online machine learning\n",
    "- ID: 7, Occurrences: 4, Title: Unsupervised learning\n",
    "- ID: 10, Occurrences: 4, Title: Reinforcement learning\n",
    "- ID: 3, Occurrences: 3, Title: Machine Learning (journal)\n",
    "- ID: 11, Occurrences: 3, Title: Meta-learning (computer science)\n",
    "- ID: 14, Occurrences: 3, Title: Curriculum learning\n",
    "- ID: 8, Occurrences: 2, Title: Weak supervision\n",
    "- ID: 9, Occurrences: 2, Title: Self-supervised learning\n",
    "- ID: 17, Occurrences: 2, Title: Neuromorphic computing\n",
    "- ID: 6, Occurrences: 1, Title: Supervised learning\n",
    "- ID: 16, Occurrences: 1, Title: Neuro-symbolic AI\n",
    "- ID: 19, Occurrences: 1, Title: Statistical classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
